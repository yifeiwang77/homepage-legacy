<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="files/favicon.ico" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Yifei Wang @ PKU</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.github.io">

  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">

  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Yifei  Wang Áéã‰∏ÄÈ£û</span>
      </div>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <!-- Intro -->
      <article class="post">
        <div class="post-content">
          <img src="files/avatar.JPG" class="profile-picture">

          <p>I am a PhD candidate (expected in 2023) at <a href="http://www.math.pku.edu.cn">School of Mathematical Sciences</a>, <a href="https://www.pku.edu.cn">Peking University</a>. I am a member of <a href="https://zero-lab-pku.github.io/">ZERO Lab</a> and advised by <a href="https://yisenwang.github.io/">Yisen Wang</a>, <a href="http://english.math.pku.edu.cn/peoplefaculty/64.html">Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>. Previously, I received a BS in mathematics and a BA in philosophy from Peking University. I am a recipient of the <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best Paper Award</a> of ECML-PKDD 2021 and the <a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a> of ICML 2021 AML workshop.
          </p>

          <p>
            My research aims at theoretical understandings and principled designs of <strong>self-supervised learning</strong>, <strong>robust learning</strong>, and <strong>graph learning</strong> algorithms. 
            <!-- I enjoy connecting the methodologies behind different learning paradigms. A different view of things brings new insights for better theoretical understandings and pratical designs. -->
            If you find my work interesting, please feel free to reach out.
          </p>
            <a href="files/yifei-wang-pku-cv.pdf"><span>CV</span></a> /
            <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
            <a href="https://twitter.com/pkuwangyifei">Twitter</a> /
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a> / 
            <a href="mailto:yifei_wang@pku.edu.cn">yifei_wang@pku.edu.cn</a>
          </p>

        </div>
      </article>
    </div>



    <div class="wrapper">
      <!-- Research -->
      <article class="post">
        <header class="post-header">
          <h1 class="post-title">Research</h1>
          <span class="footnote">(* marks equal contribution. See <a href="publication.html">here</a> for a list grouped by topic.)</span>
        </header>

        <div class="post-content">
          <ul class="publications">

            <ul class="publications">
              <li class="featured">
                <span class="title">
                How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
                </span>
                <!-- <span class="oral">SPOTLIGHT</span>               -->
                <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
                <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
                <span class="year">2022</span>
                <span class="links">
                  <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
                  <a href="https://github.com/zhangq327/U-MAE">Code</a> |
                  <a href="files/slides/NeurIPS2022_mae.pdf">Slides</a>
                </span>
            </li>
            <li class="article">
              <span class="title">
              Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
              </span>
              <span class="authors">Qixun Wang*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
              <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
                <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
                <a href="files/slides/NeurIPS2022_OOD.pdf">Slides</a>
              </span>
            </li>

            <li class="article">
            <span class="title">
              When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
              </span>
              <!-- <span class="oral">SPOTLIGHT</span>               -->
              <span class="authors">Yichuan Mo, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
              <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
                <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
              </span>
              <span class="authors">Tianqi Du*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
              <span class="journal-info">NeurIPS 2022 <a href="https://sslneurips22.github.io/">SSL Workshop</a>
              </span>
              <span class="year">2022</span>
              <!-- <span class="links">
                <a href="https://sslneurips22.github.io/">(link)</a>
              </span> -->
            </li>

            <li class="article">
              <span class="title">
              AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
              </span>
              <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
              <span class="journal-info">NeurIPS 2022 <a href="https://sslneurips22.github.io/">SSL Workshop</a> <span class="oral">(Oral)</span></span>
              <span class="year">2022</span>
              <!-- <span class="links">
                <a href="https://sslneurips22.github.io/">TBA</a>
              </span> -->
            </li>

            
            <li class="article">
              <span class="title">
                Optimization-Induced Graph Implicit Nonlinear Diffusion
                <i class='no-italics' ></i>
              </span>
              <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">ICML 2022</span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
                <a href="https://github.com/7qchen/GIND">Code</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
                <i class='no-italics' ></i>
              </span>
              <span class="authors">Mingjie Li, Xiaojun Guo, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
              <span class="journal-info">ICML 2022</span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
            </li>

            <li class="featured">
              <span class="title">
              Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
              </span>
              <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">ICLR 2022</span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
                <a href="https://github.com/zhangq327/ARC">Code</a> |
                <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
              </span>
            </li>
            <li class="featured">
              <span class="title">
                A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
              </span>
                          
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AML workshop)</span></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
                <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
              </span>
            </li>
              <li class="article">
              <span class="title">
              Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">NeurIPS 2021</span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
                <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
              </span>
            </li>
            <li class="article">
              <span class="title">
                Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">NeurIPS 2021</span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
                <a href="https://github.com/yifeiwang77/DGC">Code</a> |
                <a href="files/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
              </span>
              <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
            </li>
            <li class="article">
              <span class="title">
              Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
              </span>
                          
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info"> ECML-PKDD 2021 
                </span>
              <span class="year">2021</span>
              <span class="oral">(üèÜ Best Paper Award (1/685). Invited to <strong>Machine Learning</strong> Journal)</span>              
              <span class="links">
                <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
                <a href="https://github.com/yifeiwang77/repgan">Code</a> |
                <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
                <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
                <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
              </span>
              <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
            </li>
            <li class="article">
              <span class="title">
              Train Once, and Decode as You Like <i class='no-italics'></i>
              </span>
              <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
              <span class="journal-info">COLING 2020</span>
              <span class="year">2020</span>
              <span class="links">
                <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
              </span>
              <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> -->
            </li>
          </ul>
        </div>
        <!-- <span class="footnote">* denotes equal contribution, often meaning multiple authors contributed to coding and running experiments.</span> -->
      </article>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Selected Honors and Awards</h1>
        </header>
        <div class="post-content">
          <strong>National Scholarship</strong>, Ministry of Education of China, 2021, 2022. <br>
          <strong>Principal Scholarship</strong>, Peking University, 2022. <br>  
          <!-- <strong>Academic Innovation Award</strong>, Peking University, 2022. <br>   -->
          <!-- <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html" style="color:#000000;"> -->
            <strong>Best Paper Award </strong></a> (first authorship), ECML-PKDD 2021 (1/685), 2021.<br>
          <!-- <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> -->
            <strong>Silver Best Paper Award</strong></a> (first authorship), ICML 2021 AML workshop, 2021.<br>
        <!-- <strong>Meritorious Winner (First Prize), Mathematical Contest in Modeling (MCM), 2016. <br> -->
        <!-- <strong>Yizheng Scholarship</strong>, Peking University, 2016. -->
      </div>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Professional Services</h1>
        </header>
        <div class="post-content">
        Reviewer for NeurIPS, ICLR, ICML, CVPR, ACL, EMNLP, ECML-PKDD, etc.
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->
  

  
  <footer class="site-footer">

    <div class="wrapper">

      <div class="footer-col-wrapper">
        <div class="footer-col">

          <!-- <i>Let others boast of the pages they have written; I'm proud of the ones I've read. -- Jorge Luis Borges</i><br> -->
          <!-- <i> Error (‚Äì belief in the ideal ‚Äì) is not blindness, error is cowardice. -- Friedrich Nietzsche </i> -->
          <!-- Life is not what one lived, but what one remembers and how one remembers it in order to recount it. -- Gabriel Garc√≠a M√°rquez. -->
          <ul class="contact-list">
            <li>Yifei Wang</li>
            <li>Peking University</li>
            <!-- <li><i>Good Learning is supervised, great learning is self-supervised.</italatic></li> -->
          </ul>
        </div>
        <div class="footer-col">
        </div>
      </div>
    </div>


  </footer>
