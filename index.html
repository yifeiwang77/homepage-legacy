<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="files/favicon.ico" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preload" href="assets/fonts/xxx.woff" as="font" type="font/woff" crossorigin>

  <title>Yifei Wang</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.me">
  <link rel="preload" href="files/font/Mukta-Light.ttf" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="files/font/Mukta-Medium.ttf" as="font" type="font/woff" crossorigin>

  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Mukta:wght@300;400&display=swap" rel="stylesheet"> -->
  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">



  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <article class="post">
        <div class="post-content">
          <img src="files/avatar.JPG" class="profile-picture">

          <p>Hi! I am Yifei Wang (Áéã‰∏ÄÈ£û in Chinese), a PhD candidate (expected to graduate in June 2023) at <a href="http://www.math.pku.edu.cn">School of Mathematical Sciences</a>, <a href="https://www.pku.edu.cn">Peking University</a>. I am a member of <a href="https://zero-lab-pku.github.io/">ZERO Lab</a> and advised by <a href="https://yisenwang.github.io/">Yisen Wang</a>, <a href="http://english.math.pku.edu.cn/peoplefaculty/64.html">Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>. Previously, I received a BS in mathematics and a BA in philosophy from Peking University. 
            <!-- I received <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best ML Paper Award</a> of ECML-PKDD 2021 and <a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a> of ICML 2021 AML workshop. -->
          </p>

          <p>
            I am generally interested in theoretical understandings and principled designs of machine learning algorithms, with primary focus on <strong>unsupervised learning</strong> (representation & generation), <strong>robust learning</strong> (adversarial & OOD robustness), and <strong>graph learning</strong> (GNN & Transformer). 
            <!-- Feel free to reach out! -->
          </p>
            <a href="files/yifei-wang-pku-cv.pdf"><span>CV</span></a> /
            <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
            <a href="https://twitter.com/pkuwangyifei">Twitter</a> /
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a> / 
            <a href="mailto:yifei_wang@pku.edu.cn">yifei_wang@pku.edu.cn</a>
          </p>

          <!-- <strong>I am actively looking for postdoc and full research positions. Weclome to drop me an email if interested.</strong> -->

        </div>
      </article>
    </div>

    <div class="wrapper">
      <!-- <article class="post"> -->
        <header class="post-header">
          <h1 class="post-title">News </h1>
        </header>
        <article class="post">

          <ul>
            <li>One paper on <a href="">AT-and-IRM connection</a> <strong style="color:#0F9D58">(Oral)</strong> accepted to AAAI 2023 </li>
            <li>Two papers on  <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">CL identifiability</a> <strong style="color:#0F9D58">(Oral)</strong> & <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">EBM-based SSL</a> accepted to NeurIPS 2022 SSL workshop
            <li>Three papers on <a href="https://arxiv.org/pdf/2210.08344">MAE theory</a> <strong style="color:#0F9D58">(Spotlight)</strong>, <a href="https://arxiv.org/pdf/2210.06807">Structured AT</a> <strong style="color:#0F9D58">(Spotlight)</strong>, and <a href="https://arxiv.org/pdf/2210.07540.pdf">AT recipe for ViTs</a> <strong style="color:#0F9D58">(Spotlight)</strong> accepted to NeurIPS 2022
            <li>Two papers on <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">nonlinear graph diffusion</a> and <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">concentrated spectral graph filters</a> accepted to ICML 2022 </li>
            <li>Two papers on <a href="http://arxiv.org/pdf/2203.13457">Contrastive Learning theory</a> and <a href="http://arxiv.org/pdf/2203.13455">AT as EBM training</a> accepted to ICLR 2022 </li>
            <li>Two papers on <a href="https://arxiv.org/pdf/2110.15348">briding invariant & equivariant SSL</a> and <a href="https://arxiv.org/pdf/2007.01356">GCN's continuous diffusion</a> accepted to NeurIPS 2021 </li>
            <li>One paper on <a href="https://openreview.net/pdf?id=U0TCTe68s41">AT as EBM training</a> accepted to ICML 2021 AML workshop and won <strong style="color:#0F9D58">Silver Best Paper Award</strong>! </li>
            <li>One paper on <a href="https://arxiv.org/pdf/2107.00352">reparameterized MCMC</a> accepted to ECML-PKDD 2021  and won <strong style="color:#0F9D58">Best ML Paper Award (1/685)</strong>!</li>
          </ul>
        </article>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
          <h1 class="post-title" id="Publications">Publications <span style="font-size:medium;letter-spacing:0.00px;">(* marks equal contribution, (co-)advised interns and students are <u>underlined</u>)</a></span> </h1>
          <!-- <span class="footnote">* marks equal contribution</span> -->
        </header> 
      </article>
    <!-- Tab links -->
    <div class="tab">
      <button class="tablinks" onclick="openCity(event, 'Selected')" id="defaultOpen">üåü Selected</button>
      <button class="tablinks" onclick="openCity(event, 'All')">üìö All</button>
      <button class="tablinks" onclick="openCity(event, 'Unsupervised')">üéÇ Unsupervised Learning</button>
      <button class="tablinks" onclick="openCity(event, 'Adversarial')">üî® Robust Learning</button>
      <button class="tablinks" onclick="openCity(event, 'Graph')">üåê Graph Learning</button>
    </div>

    <!-- Tab content -->
    <div id="Selected" class="tabcontent">
      <!-- <h3>Selected</h3> -->
      <ul class="publications">
        <li class="article">
          <span class="title">
          How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
          </span>
          <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
          <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
            <a href="https://github.com/zhangq327/U-MAE">Code</a> |
            <a href="files/slides/NeurIPS2022_mae.pdf">Slides</a>
          </span>
      </li>

        <li class="article">
          <span class="title">
          Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">ICLR 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
            <a href="https://github.com/zhangq327/ARC">Code</a> |
            <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
          </span>
        </li>

      <li class="article">
        <span class="title">
          A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
        </span>         
        <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
        <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AML workshop)</span></span>
        <span class="year">2022</span>
        <span class="links">
          <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
          <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
        </span>
      </li>

      <li class="article">
        <span class="title">
        Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
        </span>
        <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
        <span class="journal-info">NeurIPS 2021</span>
        <span class="year">2021</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
          <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
          <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
        </span>
      </li>

      <li class="article">
        <span class="title">
        Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
        </span>
                    
        <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
        <span class="journal-info"> ECML-PKDD 2021 
          </span>
        <span class="year">2021</span>
        <span class="oral">(üèÜ Best ML Paper Award (1/685). Invited to Machine Learning Journal)</span>              
        <span class="links">
          <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
          <a href="https://github.com/yifeiwang77/repgan">Code</a> |
          <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
          <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
          <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
          <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
        </span>
        <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
      </li>
    </ul>

    </div>


    <div id="All" class="tabcontent">
    <ul class="publications">
      <li class="article">
        <span class="title">
          On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
        </span>
        <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
        <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
        </span>
    </li>
     <li class="article">
        <span class="title">
        How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
        </span>
        <!-- <span class="oral">SPOTLIGHT</span>               -->
        <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
        <span class="year">2022</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
          <a href="https://github.com/zhangq327/U-MAE">Code</a> |
          <a href="files/slides/NeurIPS2022_mae.pdf">Slides</a>
        </span>
    </li>
    <li class="article">
      <span class="title">
      Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
        <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
        <a href="files/slides/NeurIPS2022_OOD.pdf">Slides</a>
      </span>
    </li>

    <li class="article">
    <span class="title">
      When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
      </span>
      <!-- <span class="oral">SPOTLIGHT</span>               -->
      <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
        <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop
      </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
      AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
      </span>
    </li>

    
    <li class="article">
      <span class="title">
        Optimization-Induced Graph Implicit Nonlinear Diffusion
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
        <a href="https://github.com/7qchen/GIND">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
    </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
      </span>         
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AML workshop)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
        <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
      </span>
    </li>
    <li class="article">
      <span class="title">
        Fooling Adversarial Training with Inducing Noise
        <i class='no-italics'></i>
      </span>
      <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">Tech report, Nov. 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
      </span>
    </li>
      <li class="article">
      <span class="title">
      Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
        <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
        <a href="https://github.com/yifeiwang77/DGC">Code</a> |
        <a href="files/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
      </span>
      <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
    </li>
    <li class="article">
      <span class="title">
      Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
      </span>
                  
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info"> ECML-PKDD 2021 
        </span>
      <span class="year">2021</span>
      <span class="oral">(üèÜ Best ML Paper Award (1/685) & Invited to <strong>Machine Learning</strong> Journal)</span>              
      <span class="links">
        <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
        <a href="https://github.com/yifeiwang77/repgan">Code</a> |
        <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
        <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
        <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
      </span>
      <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
    </li>
    <li class="article">
      <span class="title">
      Train Once, and Decode as You Like <i class='no-italics'></i>
      </span>
      <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
      <span class="journal-info">COLING 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
      </span>
      <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> -->
    </li>
    <li class="article">
      <span class="title">
        Decoder-free Robustness Disentanglement without (Additional) Supervision
        <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
      <span class="journal-info">Tech report, July 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
      </span>
      <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> -->
    </li>
  </ul>
</div>

<div id="Unsupervised" class="tabcontent">
  <ul class="publications">
    <li class="article">
      <span class="title">
      How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
      </span>
      <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
        <a href="https://github.com/zhangq327/U-MAE">Code</a> |
        <a href="files/slides/NeurIPS2022_mae.pdf">Slides</a>
      </span>
  </li>

  <li class="article">
    <span class="title">
      Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop
    </span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
    </span>
  </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>

  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>

  <li class="article">
    <span class="title">
    Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">NeurIPS 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
      <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
    </span>
                
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info"> ECML-PKDD 2021 
      </span>
    <span class="year">2021</span>
    <span class="oral">(üèÜ Best ML Paper Award (1/685). Invited to Machine Learning Journal)</span>              
    <span class="links">
      <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
      <a href="https://github.com/yifeiwang77/repgan">Code</a> |
      <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
      <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
      <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
    </span>
  </li>
  <li class="article">
    <span class="title">
    Train Once, and Decode as You Like <i class='no-italics'></i>
    </span>
    <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
    <span class="journal-info">COLING 2020</span>
    <span class="year">2020</span>
    <span class="links">
      <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
    </span>
  </li>  
</ul>
</div>


<div id="Adversarial" class="tabcontent">
  <ul class="publications">
    <li class="article">
      <span class="title">
        On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
      </span>
      <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
      <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
      </span>
  </li>
  <li class="article">
    <span class="title">
    Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
      <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
      <a href="files/slides/NeurIPS2022_OOD.pdf">Slides</a>
    </span>
  </li>

  <li class="article">
  <span class="title">
    When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
    </span>
    <!-- <span class="oral">SPOTLIGHT</span>               -->
    <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
      <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
    </span>
  </li>
  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>
  <li class="article">
    <span class="title">
      Fooling Adversarial Training with Inducing Noise
      <i class='no-italics'></i>
    </span>
    <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
    <span class="journal-info">Tech report, Nov. 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
    </span>
  </li>
  <li class="article">
    <span class="title">
      Decoder-free Robustness Disentanglement without (Additional) Supervision
      <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
    <span class="journal-info">Tech report, July 2020</span>
    <span class="year">2020</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
    </span>
  </li>
  </ul>
    </div>
    <div id="Graph" class="tabcontent">
      <ul class="publications">
        <li class="article">
          <span class="title">
            Optimization-Induced Graph Implicit Nonlinear Diffusion
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
            <a href="https://github.com/7qchen/GIND">Code</a>
          </span>
        </li>
      
        <li class="article">
          <span class="title">
            G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
        </li>
        <li class="article">
          <span class="title">
            Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
          </span>
          <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">NeurIPS 2021</span>
          <span class="year">2021</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
            <a href="https://github.com/yifeiwang77/DGC">Code</a> |
            <a href="files/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
            <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
          </span>
        </li>
      </ul>
        </div>
      </div>
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Awards</h1>
        </header>
        <div class="post-content">
          <strong><a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best ML Paper Award</a></strong> (1/685), ECML-PKDD, 2021 <br>
          <!-- <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> -->
            <strong><a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a></strong>, ICML AML workshop, 2021<br>
          <strong>National Scholarship</strong>, Ministry of Education of China, 2021, 2022 <br>
          <strong>Principal Scholarship</strong>, Peking University, 2022 <br>
          <strong>Baidu Scholarship Nomination Award</strong> (20 worldwide), Baidu Inc, 2022 <br>
          <!-- <strong>Academic Innovation Award</strong>, Peking University, 2022. <br>   -->
          <!-- <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html" style="color:#000000;"> -->
      </div>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Professional Services</h1>
        </header>
        <div class="post-content">
        Reviewer for NeurIPS, ICLR, ICML, CVPR, ACL, EMNLP, ECML-PKDD
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->

<script>
  // Check if API exists
  if (document && document.fonts) {    
    // Do not block page loading
    setTimeout(function () {           
      document.fonts.load('16px "Mukta"').then(() => {
        // Make font using elements visible
        document.documentElement.classList.add('font-loaded') 
      })
    }, 0)
  } else {
    // Fallback if API does not exist 
    document.documentElement.classList.add('font-loaded') 
  }
</script>

<script>
  function openCity(evt, cityName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(cityName).style.display = "block";
    evt.currentTarget.className += " active";
  }
// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
  </script>

  </footer>
