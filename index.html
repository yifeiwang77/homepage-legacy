<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="files/favicon.ico" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Yifei Wang (PKU)</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.github.io">

  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">

  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Yifei  Wang Áéã‰∏ÄÈ£û</span>
      </div>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <!-- Intro -->
      <article class="post">
        <div class="post-content">
          <img src="files/avatar.JPG" class="profile-picture">

          <p>I am a PhD candidate (expected in 2023) at <a href="http://www.math.pku.edu.cn">School of Mathematical Sciences</a>, <a href="https://www.pku.edu.cn">Peking University</a>. I am a member of <a href="https://zero-lab-pku.github.io/">ZERO Lab</a> and advised by <a href="https://yisenwang.github.io/">Yisen Wang</a>, <a href="http://english.math.pku.edu.cn/peoplefaculty/64.html">Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>. Previously, I received a BS in mathematics and a BA in philosophy from Peking University. 
          </p>

          <p>
            My research covers <strong>self-supervised learning</strong>, <strong>adversarial learning</strong>, and <strong>graph learning</strong>. 
            <!-- I seek to discover new perspectives for better theoretical understanding and pratical design of learning algorithms. -->
            I enjoy connecting the methodologies behind different learning paradigms. A different view of things brings new insights for better theoretical understandings and pratical designs.
            <!-- and designs.  -->
            <!-- problem formulations and  across these different domains. 
             -->
            <!-- A different way of framing a problem can lead to different research questions; a different way of applying a method can help develop novel use cases.  -->
            If you find my research interesting, please feel free to reach out.
          </p>

        </div>

          <div class="post-content">
          <p> Contact: yifei_wang at pku.edu.cn / 
            <!-- <span style="color:darkslateblue">Contact: yifei_wang AT pku.edu.cn</span> /  -->
            <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
            <!-- <a href="files/wechat_QR.jpg"><span><i style="font-size:20px" class="fa">&#xf1d7;</i></span></a> / -->
            <a href="https://twitter.com/pkuwangyifei">Twitter</a>            
          </p>
          <p> Profile: 
            <a href="files/cv-yifeiwang-2022-06.pdf"><span> Curriculum Vitae (CV)</span></a> /
            <a href="https://openreview.net/profile?id=~Yifei_Wang1"><span>OpenReview</span></a> /
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a>
            <!-- <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span style="color: blue">G</span><span style="color:red">o</span><span style="color:orange">o</span><span style="color:blue">g</span><span style="color:green">l</span><span style="color:red">e</span> <span style="color:grey">Scholar</span></a> -->
          </p>
          <!-- <p>  -->
            <p>
              <strong style="color:#cc0000">I am on job market now. Weclome to email me if interested.</strong>
              <!-- <span class="oral">I am actively looking for research jobs. Please email me if interested:)</span>   -->
            </p>         
        </div>
      </article>
    </div>

    <!-- <div class="post-content"> -->
    <!-- </div> -->

    <!-- <div class="wrapper">
      Research
      <article class="post">
        <header class="post-header">
          <h1 class="post-title">News</h1>
        </header>
        <ul>
          <li> [2022.06] Awarded the President's Fellowship of Peking University.
          <li> [2022.05] Two papers (GIND and G2CN) got accepted by ICML 2022.
          <li> [2022.01] Two papers (overlap theory and CEM) got accepted by ICLR 2022.
          <li> [2021.11] Awarded the National Fellowship for Ph.D. student.
          <li> [2021.09] Two papers (DGC and Prelax) got accepted by NeurIPS 2021.
          <li> [2021.08] One paper (REP-GAN) got accepted by ECML-PKDD 2021 and won the Best ML Paper Award!
          <li> [2021.07] One paper (CEM) got accepted by ICML 2021 workshop and won Silver Best Paper Award!
          </ul>
      </div> 
    -->

<!-- 
    <div class="wrapper">
      <!-- Research -->
      <!-- <article class="post"> -->
        <!-- <header class="post-header"> -->
          <!-- <h1 class="post-title">Research Interests</h1> -->
        <!-- </header> -->
        <!-- <ul> -->
          <!-- <li> -->
            <!-- &nbsp <strong>Self-Supervised Representation Learning</strong> -->
            <!-- <ul>
             <li>
              tighter bounds, new theories, and model selection metrics (<a href="http://arxiv.org/pdf/2203.13457">ICLR 2022a</a>)
             </li> 
            <li>
              probabilistic formulation and sampling algorithms (<a href="http://arxiv.org/pdf/2203.13455">ICLR 2022b</a>)
            </li> 
            <li>
              bridging two complementary pretext tasks via residual relaxation (<a href="https://arxiv.org/pdf/2110.15348">NeurIPS 2021a</a>)
            </li> 
          </ul> -->
          <!-- </li>
          <li>
            &nbsp <strong>Robust (Adversarial) Representation Learning</strong> -->
            <!-- <ul>
              <li>
                a probabilistic interpretation of adversarial training (<a href="http://arxiv.org/pdf/2203.13455">ICLR 2022b</a>)
              </li>   
              <!-- <li>
                deeper understanding of robust overfitting (in submission)
              </li>
            <li>
              disentangling robust and non-robust features end-to-end (<a href="https://arxiv.org/pdf/2007.01356">arxiv</a>)
             </li> 
             <li>
              designing unlearnable examples resistant to adversarial training (<a href="https://arxiv.org/pdf/2111.10130">arxiv</a>)
            </li> 
            </ul> -->
          <!-- </li>
          <li>
            &nbsp <strong>Representation Learning on Graphs </strong> -->
            <!-- <ul>
             <li>
              a continuous reformulation of GCN feature propagation (<a href="https://arxiv.org/pdf/2102.10739">NeurIPS 2021b</a>)
             </li>
             <li> 
              designing implicit nonlinear diffusion from an optimization perspective (<a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">ICML 2022a</a>)
            </li> 
             <li>
              spectral analysis of existing graph filters and designing flexible basis (<a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">ICML 2022b</a>)
            </li> 
          </ul> -->
          <!-- </li>
          <li>
            &nbsp <strong>Probabilistic Modeling and Inference</strong> -->
            <!-- <ul>
              <li>
                principled MCMC sampling from adversarially robust models (<a href="http://arxiv.org/pdf/2203.13455">ICLR 2022b</a>)
              </li>              
              <li>
                reparameterized MCMC sampling for GANs (<a href="https://arxiv.org/pdf/2107.00352">ECML-PKDD 2021</a>)
              </li>              
              <li>
              designing autoregressive models adaptive to arbitrary decoding orders (<a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">COLING 2021</a>)
              </li> 
          </ul> -->
          <!-- </li>
        </ul>
    </div> -->

    <div class="wrapper">
      <!-- Research -->
      <article class="post">
        <header class="post-header">
          <h1 class="post-title">Publication</h1>
          <span class="footnote">(* marks equal contribution. See <a href="publication.html">here</a>  for a full list grouped by topic.)</span>
        </header>

        <div class="post-content">
          <ul class="publications">

            <ul class="publications">
              <li class="article">
                <span class="title">
                How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
                </span>
                <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
                <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2022)</strong></span>
                <span class="year">2022</span>
                <span class="links">
                  <a href="">PDF</a>
                </span>
            </li>
            <li class="article">
              <span class="title">
              Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
              </span>
              <span class="authors">Qixun Wang*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="">PDF</a>
              </span>
            </li>

            <li class="article">
            <span class="title">
              Improving Adversarial Robustness of Vision Transformers  <i class='no-italics'>  </i>
              </span>
              <span class="authors">Yichuan Mo, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="">PDF</a>
              </span>
            </li>
            <li class="article">
              <span class="title">
                Optimization-Induced Graph Implicit Nonlinear Diffusion
                <i class='no-italics' ></i>
              </span>
              <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Machine Learning <strong>(ICML 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
                <a href="https://github.com/7qchen/GIND">Code</a>
              </span>
            </li>

            <li class="article">
              <span class="title">
                G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
                <i class='no-italics' ></i>
              </span>
              <span class="authors">Mingjie Li, Xiaojun Guo, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Machine Learning <strong>(ICML 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
            </li>

            <li class="article">
              <span class="title">
              Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning <br> via Augmentation Overlap <i class='no-italics' ></i>
              </span>
              <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Learning Representations <strong>(ICLR 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
                <a href="https://github.com/zhangq327/ARC">Code</a> |
                <a href="files/slides/ICLR2022_overlap.pdf">Slides</a>
              </span>
            </li>
            <li class="article">
              <span class="title">
                A Unified Contrastive Energy-based Model for Understanding the Generative Ability  <br> of Adversarial Training <i class='no-italics'>  </i>
              </span>
              <span class="oral">üèÜ Silver Best Paper Award @ ICML 2021 AML workshop</span>              
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">International Conference on Learning Representations <strong>(ICLR 2022)</strong></span>
              <span class="year">2022</span>
              <span class="links">
                <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
                <a href="files/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
              </span>
            </li>
              <li class="article">
              <span class="title">
              Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2021)</strong></span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
                <a href="files/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
              </span>
              <!-- <span class="intro" hidden="hidden">Do we really need exact alignment of different views in contrastive learning? We show this could be 1) problematic and 2) resolved through a residual relaxation mechanism.</span> -->
            </li>
            <li class="article">
              <span class="title">
                Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
              </span>
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">Advances in Neural Information Processing Systems <strong>(NeurIPS 2021)</strong></span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
                <a href="https://github.com/yifeiwang77/DGC">Code</a> |
                <a href="files/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
              </span>
              <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
            </li>
            <li class="article">
              <span class="title">
              Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
              </span>
              <span class="oral">üèÜ Best Machine Learning Paper Award </span>              
              <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
              <span class="journal-info">European Conference on Machine Learning <strong>(ECML-PKDD 2021)</strong>
                </span>
              <span class="year">2021</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
                <a href="https://github.com/yifeiwang77/repgan">Code</a> |
                <a href="files/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
                <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
                <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
                <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
              </span>
              <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
            </li>
            <li class="article">
              <span class="title">
              Train Once, and Decode as You Like <i class='no-italics'></i>
              </span>
              <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
              <span class="journal-info">International Committee on Computational Linguistics <strong>(COLING 2020)</strong> </span>
              <span class="year">2020</span>
              <span class="links">
                <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
              </span>
              <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive)</strong>. </span> -->
            </li>
          </ul>
        </div>
        <!-- <span class="footnote">* denotes equal contribution, often meaning multiple authors contributed to coding and running experiments.</span> -->
      </article>
    </div>
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Academic Services</h1>
        </header>
        <div class="post-content">
        Reviewer for ICLR, ICML, NeurIPS, ACL, EMNLP, ECML-PKDD, etc.
      </div>
    </div>
    
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Selected Honors and Awards</h1>
        </header>
        <div class="post-content">
          <strong>National Fellowship</strong>, Ministry of Education, 2021, 2022. <br>
          <strong>President's Fellowship</strong>, Peking University, 2022. <br>  
          <!-- <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html" style="color:#000000;"> -->
            <strong>Best (Student) Machine Learning Paper Award </strong></a> <i>(first authorship)</i>, ECML-PKDD 2021.<br>
          <!-- <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> -->
            <strong>Silver Best Paper Award</strong></a> <i>(first authorship)</i>, ICML 2021 AML workshop.<br>
        <strong>Meritorious Winner (First Prize)</strong>, Mathematical Contest in Modeling (MCM), 2016. <br>
        <!-- <strong>Scientific Research Award</strong>, Peking University, 2021. -->
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->
  

  
  <footer class="site-footer">

    <div class="wrapper">

      <div class="footer-col-wrapper">
        <div class="footer-col">

          <!-- <i>Let others boast of the pages they have written; I'm proud of the ones I've read. -- Jorge Luis Borges</i><br> -->
          <!-- <i> Error (‚Äì belief in the ideal ‚Äì) is not blindness, error is cowardice. -- Friedrich Nietzsche </i> -->
          <!-- Life is not what one lived, but what one remembers and how one remembers it in order to recount it. -- Gabriel Garc√≠a M√°rquez. -->
          <ul class="contact-list">
            <li>Yifei Wang</li>
            <li>Peking University</li>
            <!-- <li><i>Good Learning is supervised, great learning is self-supervised.</italatic></li> -->
          </ul>
        </div>
        <div class="footer-col">
        </div>
      </div>
    </div>


  </footer>
