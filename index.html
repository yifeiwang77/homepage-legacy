<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="assets/avatar.JPG" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preload" href="assets/fonts/xxx.woff" as="font" type="font/woff" crossorigin>

  <title>Yifei Wang</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.me">
  <link rel="preload" href="assets/font/Mukta-Light.ttf" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="assets/font/Mukta-Medium.ttf" as="font" type="font/woff" crossorigin>

  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Mukta:wght@300;400&display=swap" rel="stylesheet"> -->
  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">


  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Yifei Wang (<span style="font-family:'Kaiti SC'">Áéã‰∏ÄÈ£û</span>)</span>
      </div>
    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <article class="post">
        <div class="post-content">
          <img src="assets/avatar.JPG" class="profile-picture">

          <p> Hello! I am a postdoctoral researcher at MIT <a href="https://www.csail.mit.edu/">CSAIL</a> working with <a href="https://people.csail.mit.edu/stefje/">Stefanie Jegelka</a>. 
             I obtained my PhD in Applied Mathematics from <a href="https://www.pku.edu.cn">Peking University (PKU)</a> in 2023, advised by <a href="https://yisenwang.github.io/">Yisen Wang</a>, <a href="https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm">Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>. My works received <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best ML Paper Award</a> of ECML-PKDD 2021 and <a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a> of ICML 2021 AdvML workshop. 
          </p>

          <p>
          I am generally interested in developing theoretical understandings and principled designs for machine learning algorithms. 
          <!-- I work on <strong>self-supervised learning</strong>, <strong>adversarial learning</strong> (adversarial & OOD robustness), and <strong>graph learning</strong> (GNNs & Transformers), and  -->
          I care the most about the following learning problems:
          <ul>
            <li><strong style="color:#0F9D58">What features we should learn:</strong> different learning objectives lead to different features. I am mostly interested in <strong>Self-Supervised Learning (SSL)</strong> that allows large-scale learning with massive unlabeled data.</li>
            <li><strong style="color:#0F9D58">How features are learned:</strong> each NN has its own feature dynamics inside. I am interested in the mechanisms of the more "interpretable" ones with symmetry priors, such as, <strong>Graph Neural Networks</strong> and <strong>Transformers</strong>.</li>
            <li><strong style="color:#0F9D58">How robust are learned models:</strong> real-world deployment requires model robustness. I explore how model vulenerabilty arises and how to build models <strong>robust to adversarial attacks and distribution shifts</strong>. </li>
          </ul>
          <!-- Feel free to shoot me an email if you are interested in working with me. -->
         </p>

        </div>
        <div class="post-content">
          <!-- <strong> -->
          <p>
            Contact: yifei_w at mit.edu / 
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a> /
             <!-- <a href="assets/cv_yifei_wang_oct23.pdf"><span>CV (outdated)</span></a> / -->
             <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
             <a href="https://x.com/yifeiwang77">X (Twitter)</a>
           </p>
          <!-- </strong> -->
        </div>
    
      </article>
    </div>

    <div class="wrapper">
      <!-- <article class="post"> -->
        <header class="post-header">
          <h1 class="post-title">News </h1>
        </header>
        <article class="post">

          <ul>
            <li>2024.01. üî• Three papers on understanding and designing modern SSL methods were accepted by <strong>ICLR 2024</strong>, covering 1) <a href="https://openreview.net/pdf?id=lNCnZwcH5Z">bridging NMF and contrastive learning to gain feature intepretability</a>, 2)  <a href="https://openreview.net/pdf?id=S5EqslEHnz">how contrastive learning benefits from synthetic data</a>, and 3) <a href="https://openreview.net/pdf?id=WNLAkjUm19">how discrete tokens help representation learning <strong style="color:#0F9D58">(Spotlight)</strong></a>. See you in Vienna! üé°</li>
            <li>2023.12. I have joined MIT CSAIL as a postdoc.</li>
            <li>2023.09. Five papers were accepted by <strong>NeurIPS 2023</strong>, aiming to provide better understandings of <a href="https://arxiv.org/pdf/2311.02687.pdf">graph CL</a>, <a href="https://arxiv.org/pdf/2310.18904.pdf">identifiability of CL</a>, <a href="https://arxiv.org/pdf/2310.18936.pdf">adversarial examples</a>, <a href="https://arxiv.org/pdf/2310.19360.pdf">robust overfitting</a>, and <a href="https://arxiv.org/pdf/2310.18716.pdf">canonicalizability of graph eigenvectors</a>. Paper and code are released.   </li>
            <li>2023.09. I will be serving as an area chair for <strong>ICLR 2024</strong>.</li>
            <li>2023.05. Two SSL papers were accepted by <strong>ICML 2023</strong>, which analyze the roles of <a href="https://arxiv.org/pdf/2306.04272">multi-modal supervision (e.g., image-text pairs in CLIP)</a> and <a href="https://arxiv.org/pdf/2306.04160">weak supervision (e.g., noisy labels)</a> in self-supervised learning.</li>
            <li>2023.02. One paper on <a href="https://arxiv.org/pdf/2303.14460.pdf">fairness in adversarial training</a> was accepted by CVPR 2023.</li> 
            <!-- <li>One paper on <a href="https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf">equilibrium-seeking image denoisor</a> was accepted by IEEE Transaction on Image Processing (TIP).</li> -->
            <li>2023.01. Five papers about contrastive learning (CL) and graph learning were accepted by <strong>ICLR 2023</strong>, covering <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">training dynamics</a>, <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">dimensional collapse</a>, <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">dynamic augmentation</a>, <a href="https://openreview.net/pdf?id=SM7XkJouWHm">CL-inspired normalization</a>, and <a href="https://openreview.net/pdf?id=j3cUWIMsFBN">unbiased graph sampling</a>. </li>
            <!-- <li> Two papers were also accepted by ICLR 2023 workshops, covering <a href="https://openreview.net/pdf?id=Noj1Fydegod">unsupervised backdoor defense</a> (<a href="https://iclr23-bands.github.io/">ICLR'23 BANDS</a>) and <a href="https://openreview.net/pdf?id=T-NiH_wB1O">transferability of contrastive learning</a> (<a href="">ICLR'23 ME-Fomo</a>)</li> -->
            <!-- <li>One paper on <a href="">AT-and-IRM connection</a> <strong style="color:#0F9D58">(Oral)</strong> was accepted by AAAI 2023 </li> -->
            <!-- <li>Three papers were accepted by <strong>NeurIPS 2022</strong>, covering <a href="https://arxiv.org/pdf/2210.08344">MAE theory</a> <strong style="color:#0F9D58">(Spotlight)</strong>, <a href="https://arxiv.org/pdf/2210.06807">Structured AT</a> <strong style="color:#0F9D58">(Spotlight)</strong>, and <a href="https://arxiv.org/pdf/2210.07540.pdf">AT recipe for ViTs</a> <strong style="color:#0F9D58">(Spotlight)</strong> -->
            <!-- <li>Two papers on  <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">CL identifiability</a> <strong style="color:#0F9D58">(Oral)</strong> & <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">EBM-based SSL</a> were accepted by NeurIPS 2022 SSL workshop
            <li>Two papers on <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">nonlinear graph diffusion</a> and <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">concentrated spectral graph filters</a> were accepted by ICML 2022 </li>
            <li>Two papers on <a href="http://arxiv.org/pdf/2203.13457">Contrastive Learning theory</a> and <a href="http://arxiv.org/pdf/2203.13455">AT as EBM training</a> were accepted by ICLR 2022 </li>
            <li>Two papers on <a href="https://arxiv.org/pdf/2110.15348">briding invariant & equivariant SSL</a> and <a href="https://arxiv.org/pdf/2007.01356">GCN's continuous diffusion</a> were accepted by NeurIPS 2021 </li>
            <li>One paper on <a href="https://openreview.net/pdf?id=U0TCTe68s41">AT as EBM training</a> was accepted by ICML 2021 AdvML workshop and won <strong style="color:#0F9D58">Silver Best Paper Award</strong>! </li>
            <li>One paper on <a href="https://arxiv.org/pdf/2107.00352">reparameterized MCMC</a> was accepted by ECML-PKDD 2021 and won <strong style="color:#0F9D58">Best ML Paper Award (1/685)</strong>!</li>  -->
          </ul>
        </article>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
          <h1 class="post-title" id="Publications">Publications <span style="font-size:14px;letter-spacing:0.00px;">(* marks equal contribution)</a></span> </h1>
        </header> 
      </article>
    <!-- Tab links -->
    <div class="tab">
      <button class="tablinks" disabled><i>Topics:</i></button>
      <button class="tablinks" onclick="openCity(event, 'All')"  id="defaultOpen">üìö All</button>
      <!-- <button class="tablinks" onclick="openCity(event, 'Selected')"   id="defaultOpen">üåü Selected</button> -->
      <!-- <button class="tablinks" onclick="openCity(event, 'All')"  id="defaultOpen">üìö All Publications</button> -->
      <!-- <button class="tablinks" onclick="openCity(event, 'Unsupervised')">Topics:</button> -->
      <button class="tablinks" onclick="openCity(event, 'Unsupervised')">üéÇ Self-supervised Learning</button>
      <button class="tablinks" onclick="openCity(event, 'Adversarial')">üî® Adversarial Learning</button>
      <button class="tablinks" onclick="openCity(event, 'Graph')">üåê Graph Learning</button>
    </div>

    <div id="All" class="tabcontent">
      <ul class="publications">

      <li class="article">
        <span class="title">
          Non-negative Contrastive Learning
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yaoyu Guo, Yisen Wang</span>
        <span class="journal-info">ICLR 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=lNCnZwcH5Z">PDF</a>
        </span>
        </li>

      <li class="article">
        <span class="title">
          Do Generated Data Always Help Contrastive Learning?
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, Jizhe Zhang*, Yisen Wang</span>
        <span class="journal-info">ICLR 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=S5EqslEHnz">PDF</a>
        </span>
        </li>

        <li class="article">
          <span class="title">
            On the Role of Discrete Tokenization in Visual Representation Learning
          </span>
          <span class="authors">Tianqi Du, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">ICLR 2024 <strong style="color:#0F9D58">(Spotlight)</strong></span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=WNLAkjUm19">PDF</a>
          </span>
          </li>

        <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>

        <li class="article">
          <span class="title">
            Identifiable Contrastive Learning with Automatic Feature Importance Discovery
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18904.pdf">PDF</a> | 
            <a href="https://github.com/PKU-ML/Tri-factor-Contrastive-Learning">Code</a>            
          </span>
          </li>

          <li class="article">
          <span class="title">
            Adversarial Examples Are Not Real Features 
          </span>
          <span class="authors">Ang Li*, <strong>Yifei Wang*</strong>, Yiwen Guo, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18936.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/AdvNotRealFeatures">Code</a>          
          </span>
          </li>
  
        <li class="article">
          <span class="title">
            Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Liangchen Li*, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.19360.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/ReBAT">Code</a>          
          </span>
          </li>
  

        <li class="article">
          <span class="title">
            Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding 
          </span>
          <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18716.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/LaplacianCanonization">Code</a>          
          </span>
          </li>          
        
        <li class="article">
          <span class="title">
            On the Generalization of Multi-modal Contrastive Learning 
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">ICML 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2306.04272">PDF</a> | 
            <a href="https://github.com/PKU-ML/CLIP-Help-SimCLR">Code</a>
          </span>
          </li>


          <li class="article">
            <span class="title">
              Rethinking Weak Supervision in Helping Contrastive Representation Learning
            </span>
            <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
            <span class="journal-info">ICML 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2306.04160">PDF</a>
            </span>
          </li>

            <li class="article">
              <span class="title">
                CFA: Class-wise Calibrated Fair Adversarial Training
              </span>
              <span class="authors">Zeming Wei, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
              <span class="journal-info">CVPR 2023</span>
              <span class="year">2023</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2303.14460.pdf">PDF</a> |
                <a href="https://github.com/PKU-ML/CFA">Code</a>
              </span>
        
        <li class="article">
          <span class="title">
            Equilibrium Image Denoising with Implicit Differentiation
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Zhengyang Geng, Yisen Wang, Jiansheng Yang, and Zhouchen Lin</span>
          <span class="journal-info">IEEE Transactions on Image Processing (TIP)</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf">PDF</a>
          </span>
      </li>

        <li class="article">
          <span class="title">
            A Message Passing Perspective on Learning Dynamics of Contrastive Learning
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, <u>Tianqi Du</u>, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">PDF</a> |
            <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning">Code</a> |
            <a href="assets/slides/ICLR23_Message_Passing.pdf">Slides</a> |
            <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg">Blog</a>
          </span>
      </li>

      <li class="article">
        <span class="title">
          Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism
        </span>
        <span class="authors"><u>Zhijian Zhuo*</u>, <strong>Yifei Wang*</strong>, Jinwen Ma, Yisen Wang </span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">PDF</a> | 
          <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism">Code</a>
        </span>
    </li>

      <li class="article">
        <span class="title">
          Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
        </span>
        <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a> |
          <a href="https://github.com/PKU-ML/DynACL">Code</a>
        </span>
    </li>

    <li class="article">
      <span class="title">
        ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
      </span>
      <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a> | 
        <a href="https://github.com/PKU-ML/ContraNorm">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States
      </span>
      <span class="authors">Mingjie Li, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=j3cUWIMsFBN">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        What Contrastive Learning Learns Beyond Class-wise Features?
      </span>
      <span class="authors">Xingyuming Liu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=T-NiH_wB1O">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Rethinking the Necessity of Labels in Backdoor Defense 
      </span>
      <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
      </span>
    </li>


      <li class="article">
        <span class="title">
          On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
        </span>
        <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
        <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
        </span>
    </li>
     <li class="article">
        <span class="title">
        How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
        </span>
        <!-- <span class="oral">SPOTLIGHT</span>               -->
        <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
        <span class="year">2022</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
          <a href="https://github.com/zhangq327/U-MAE">Code</a> |
          <a href="assets/slides/NeurIPS2022_mae.pdf">Slides</a>
        </span>
    </li>
    <li class="article">
      <span class="title">
      Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
        <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
        <a href="assets/slides/NeurIPS2022_OOD.pdf">Slides</a>
      </span>
    </li>

    <li class="article">
    <span class="title">
      When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
      </span>
      <!-- <span class="oral">SPOTLIGHT</span>               -->
      <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
        <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop
      </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
      AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jianlong Chang, Qi Tian, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">IEEE BigData 2022 <span class="oral">(Long Talk)</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="assets/papers/IEEE-BigData22-VEQ.pdf">PDF</a>
      </span>
    </li>
    
    <li class="article">
      <span class="title">
        Optimization-Induced Graph Implicit Nonlinear Diffusion
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
        <a href="https://github.com/7qchen/GIND">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
    </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="assets/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
      </span>         
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
        <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
      </span>
    </li>
    <!-- <li class="article">
      <span class="title">
        Fooling Adversarial Training with Inducing Noise
        <i class='no-italics'></i>
      </span>
      <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">Tech report, Nov. 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
      </span>
    </li> -->
      <li class="article">
      <span class="title">
      Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
        <a href="assets/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
        <a href="https://github.com/yifeiwang77/DGC">Code</a> |
        <a href="assets/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
      </span>
      <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
    </li>
    <li class="article">
      <span class="title">
      Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
      </span>
                  
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info"> ECML-PKDD 2021 
        </span>
      <span class="year">2021</span>
      <span class="oral">(üèÜ Best ML Paper Award (1/685) & Invited to <strong>Machine Learning</strong> Journal)</span>              
      <span class="links">
        <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
        <a href="https://github.com/yifeiwang77/repgan">Code</a> |
        <a href="assets/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
        <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
        <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
      </span>
      <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
    </li>
    <li class="article">
      <span class="title">
      Train Once, and Decode as You Like <i class='no-italics'></i>
      </span>
      <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
      <span class="journal-info">COLING 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
      </span>
      <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> -->
    </li>
    <!-- <li class="article">
      <span class="title">
        Decoder-free Robustness Disentanglement without (Additional) Supervision
        <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
      <span class="journal-info">Tech report, July 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
      </span>
       <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> 
    </li> -->
  </ul>
</div>

<div id="Unsupervised" class="tabcontent">
  <ul class="publications">


    <li class="article">
      <span class="title">
        Non-negative Contrastive Learning
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yaoyu Guo, Yisen Wang</span>
      <span class="journal-info">ICLR 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=lNCnZwcH5Z">PDF</a>
      </span>
      </li>

    <li class="article">
      <span class="title">
        Do Generated Data Always Help Contrastive Learning?
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Jizhe Zhang*, Yisen Wang</span>
      <span class="journal-info">ICLR 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=S5EqslEHnz">PDF</a>
      </span>
      </li>

      <li class="article">
        <span class="title">
          On the Role of Discrete Tokenization in Visual Representation Learning
        </span>
        <span class="authors">Tianqi Du, <strong>Yifei Wang*</strong>, Yisen Wang </span>
        <span class="journal-info">ICLR 2024 <strong style="color:#0F9D58">(Spotlight)</strong></span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=WNLAkjUm19">PDF</a>
        </span>
        </li>

          <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>
  

        <li class="article">
          <span class="title">
            Identifiable Contrastive Learning with Automatic Feature Importance Discovery
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18904.pdf">PDF</a> | 
            <a href="https://github.com/PKU-ML/Tri-factor-Contrastive-Learning">Code</a>            
          </span>
          </li>

    <li class="article">
      <span class="title">
        On the Generalization of Multi-modal Contrastive Learning 
      </span>
      <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
      <span class="journal-info">ICML 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2306.04272">PDF</a> | 
        <a href="https://github.com/PKU-ML/CLIP-Help-SimCLR">Code</a>
      </span>
      </li>


      <li class="article">
        <span class="title">
          Rethinking Weak Supervision in Helping Contrastive Representation Learning
        </span>
        <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">ICML 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2306.04160">PDF</a>
        </span>
      </li>
    
      <li class="article">
        <span class="title">
          A Message Passing Perspective on Learning Dynamics of Contrastive Learning
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, <u>Tianqi Du</u>, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">PDF</a> |
          <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning">Code</a> |
          <a href="assets/slides/ICLR23_Message_Passing.pdf">Slides</a> |
          <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg">Blog</a>          
        </span>
    </li>

    <li class="article">
      <span class="title">
        Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism
      </span>
      <span class="authors"><u>Zhijian Zhuo*</u>, <strong>Yifei Wang*</strong>, Jinwen Ma, Yisen Wang </span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">PDF</a> | 
        <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism">Code</a>
      </span>
  </li>

    <li class="article">
      <span class="title">
        Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
      </span>
      <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a> |
        <a href="https://github.com/PKU-ML/DynACL">Code</a>
      </span>
  </li>

  <li class="article">
    <span class="title">
      ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
    </span>
    <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
    <span class="journal-info">ICLR 2023</span>
    <span class="year">2023</span>
    <span class="links">
      <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a> | 
      <a href="https://github.com/PKU-ML/ContraNorm">Code</a>
    </span>
  </li>

      <li class="article">
      <span class="title">
        What Contrastive Learning Learns Beyond Class-wise Features?
      </span>
      <span class="authors">Xingyuming Liu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=T-NiH_wB1O">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Rethinking the Necessity of Labels in Backdoor Defense 
      </span>
      <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
      How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
      </span>
      <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
        <a href="https://github.com/zhangq327/U-MAE">Code</a> |
        <a href="assets/slides/NeurIPS2022_mae.pdf">Slides</a>
      </span>
  </li>

  <li class="article">
    <span class="title">
      Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop
    </span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
    </span>
  </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="assets/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>

  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>

  <li class="article">
    <span class="title">
    Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">NeurIPS 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
      <a href="assets/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
    </span>
                
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info"> ECML-PKDD 2021 
      </span>
    <span class="year">2021</span>
    <span class="oral">(üèÜ Best ML Paper Award (1/685). Invited to Machine Learning Journal)</span>              
    <span class="links">
      <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
      <a href="https://github.com/yifeiwang77/repgan">Code</a> |
      <a href="assets/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
      <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
      <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Award</a>
    </span>
  </li>
</ul>
</div>


<div id="Adversarial" class="tabcontent">  
  <ul class="publications">
  
      <li class="article">
        <span class="title">
          Adversarial Examples Are Not Real Features 
        </span>
        <span class="authors">Ang Li*, <strong>Yifei Wang*</strong>, Yiwen Guo, Yisen Wang </span>
        <span class="journal-info">NeurIPS 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2310.18936.pdf">PDF</a> |
          <a href="https://github.com/PKU-ML/AdvNotRealFeatures">Code</a>          
        </span>
        </li>

        <li class="article">
          <span class="title">
            Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Liangchen Li*, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.19360.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/ReBAT">Code</a>          
          </span>
          </li>


    <li class="article">
      <span class="title">
        CFA: Class-wise Calibrated Fair Adversarial Training
      </span>
      <span class="authors">Zeming Wei, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
      <span class="journal-info">CVPR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2303.14460.pdf">PDF</a> |
                <a href="https://github.com/PKU-ML/CFA">Code</a>
      </span>
  
    <li class="article">
      <span class="title">
        Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
      </span>
      <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a>
      </span>
  </li>        

  <li class="article">
    <span class="title">
      Rethinking the Necessity of Labels in Backdoor Defense 
    </span>
    <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
    <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
    <span class="year">2023</span>
    <span class="links">
      <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
    </span>
  </li>

    <li class="article">
      <span class="title">
        On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
      </span>
      <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
      <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
      </span>
  </li>
  <li class="article">
    <span class="title">
    Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
      <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
      <a href="assets/slides/NeurIPS2022_OOD.pdf">Slides</a>
    </span>
  </li>

  <li class="article">
  <span class="title">
    When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
    </span>
    <!-- <span class="oral">SPOTLIGHT</span>               -->
    <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
      <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
    </span>
  </li>
  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>
  <!-- <li class="article">
    <span class="title">
      Fooling Adversarial Training with Inducing Noise
      <i class='no-italics'></i>
    </span>
    <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
    <span class="journal-info">Tech report, Nov. 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
    </span>
  </li> -->
  <!-- <li class="article">
    <span class="title">
      Decoder-free Robustness Disentanglement without (Additional) Supervision
      <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
    <span class="journal-info">Tech report, July 2020</span>
    <span class="year">2020</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
    </span>
  </li> -->
  </ul>
    </div>
    <div id="Graph" class="tabcontent">      
      <ul class="publications">

        <li class="article">
          <span class="title">
            Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding 
          </span>
          <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18716.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/LaplacianCanonization">Code</a>          
          </span>
          </li>

          <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>
  


        <li class="article">
          <span class="title">
            ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
          </span>
          <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a>
          </span>
        </li>
    
        <li class="article">
          <span class="title">
            Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States
          </span>
          <span class="authors">Mingjie Li, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=j3cUWIMsFBN">PDF</a>
          </span>
        </li>
    
        <li class="article">
          <span class="title">
            Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jianlong Chang, Qi Tian, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">IEEE BigData 2022 <span class="oral">(Long Talk)</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="assets/papers/IEEE-BigData22-VEQ.pdf">PDF</a>
          </span>
        </li>

        <li class="article">
          <span class="title">
            Optimization-Induced Graph Implicit Nonlinear Diffusion
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
            <a href="https://github.com/7qchen/GIND">Code</a>
          </span>
        </li>
      
        <li class="article">
          <span class="title">
            G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
        </li>
        <li class="article">
          <span class="title">
            Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
          </span>
          <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">NeurIPS 2021</span>
          <span class="year">2021</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
            <a href="https://github.com/yifeiwang77/DGC">Code</a> |
            <a href="assets/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
            <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
          </span>
        </li>
      </ul>
        </div>
      </div>
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Selected Awards</h1>
        </header>
        <div class="post-content">
          <strong>Wuwenjun Outstanding Ph.D. Dissertation Runner-Up</strong> (top 14 nation-wide), <a href="https://en.caai.cn/">CAAI</a>, 2023<br>
          <strong>Excellent Graduate</strong> (top 1 per department), Beijing, 2023<br>
          <strong>Excellent Graduate</strong>, Peking University, 2023<br>
          <strong>Baidu Scholarship Runner-Up</strong> (top 20 nation-wide), Baidu Inc, 2022<br>
          <strong>National Scholarship</strong> (top 0.1% nation-wide), China, 2021, 2022 <br>
          <strong>Principal Scholarship</strong> (top 1% university-wide), Peking University, 2022 <br>
          <strong><a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best ML Paper Award</a></strong> (1/685), ECML-PKDD, 2021 <br>
          <!-- <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> -->
            <strong><a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a></strong>, ICML AdvML workshop, 2021<br>
          <!-- <strong>Academic Innovation Award</strong>, Peking University, 2022. <br>   -->
          <!-- <a href="https://2021.ecmlpkdd.org/index.html@p=2148.html" style="color:#000000;"> -->
      </div>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Professional Services</h1>
        </header>
        <div class="post-content">
        Reviewer and/or program commitee member:
        <ul>
          <li>ML Conferences: <strong>NeurIPS</strong> (2022, 2023), <strong>ICML</strong> (2022), <strong>AISTATS</strong> (2024), <strong>LoG</strong> (2023), <strong>ECML-PKDD</strong> (2022)</li>
          <li>Other conferences: <strong>CVPR</strong> (2023, 2024), <strong>ICCV</strong> (2023), <strong>ACL</strong> (2020, 2021)</li>
        <li>Journal: <strong>IEEE TPAMI</strong>, <strong>TMLR</strong></li>
      </ul>
      Area chair:
      <ul>
        <li><strong>ICLR</strong> (2024)</li>
      </ul>
        <!-- Reviewed for NeurIPS, ICLR, ICML, TPAMI (ML), CVPR, ICCV, ACL, EMNLP, ECML-PKDD. -->
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->

<script>
  // Check if API exists
  if (document && document.fonts) {    
    // Do not block page loading
    setTimeout(function () {           
      document.fonts.load('16px "Mukta"').then(() => {
        // Make font using elements visible
        document.documentElement.classList.add('font-loaded') 
      })
    }, 0)
  } else {
    // Fallback if API does not exist 
    document.documentElement.classList.add('font-loaded') 
  }
</script>

<script>
  function openCity(evt, cityName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(cityName).style.display = "block";
    evt.currentTarget.className += " active";
  }
// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
  </script>

  </footer>
