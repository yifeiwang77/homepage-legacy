<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="icon" href="assets/avatar.JPG" type="image/x-icon">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preload" href="assets/fonts/xxx.woff" as="font" type="font/woff" crossorigin>

  <title>Yifei Wang</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="yifeiwang.me">
  <link rel="preload" href="assets/font/Mukta-Light.ttf" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="assets/font/Mukta-Medium.ttf" as="font" type="font/woff" crossorigin>

  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Mukta:wght@300;400&display=swap" rel="stylesheet"> -->
  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">


  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Yifei Wang (<span style="font-family:'Kaiti SC'">Áéã‰∏ÄÈ£û</span>)</span>
      </div>
    </div>
  </header>


  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <article class="post">
        <div class="post-content">
          <img src="assets/avatar.JPG" class="profile-picture">
          <p><br></p>

          <p> Hello! I am a postdoctoral researcher at MIT <a href="https://www.csail.mit.edu/">CSAIL</a> working with <a href="https://people.csail.mit.edu/stefje/">Stefanie Jegelka</a>. 
             I obtained my PhD in Applied Mathematics from <a href="https://www.pku.edu.cn">Peking University (PKU)</a> in 2023, advised by <a href="https://yisenwang.github.io/">Yisen Wang</a>, <a href="https://www.math.pku.edu.cn/jsdw/js_20180628175159671361/y_20180628175159671361/69984.htm">Jiansheng Yang</a>, and <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>. Prior to that, I got my bachelor's degrees from PKU math and phil.<br>
             My first-author works received <a href="https://ecmlpkdd.org/2021/">Best ML Paper Award</a> of ECML-PKDD 2021, <a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a> of ICML 2021 AdvML workshop, and <a href="iclworkshop.github.io">Spotlight Award</a> at ICML 2024 ICL workshop</a>. 
          </p>
             <p>
          I am generally interested in machine learning and representation learning, with a focus on the following topics:
           <!-- mostly interested in developing theoretical understandings and principled designs of foundation models from the following key aspects: -->
           <!-- machine learning in the following scenarios: -->
          <!-- machine learning algorithms, especia paying attention to -->
          <!-- I work on <strong>self-supervised learning</strong>, <strong>adversarial learning</strong> (adversarial & OOD robustness), and <strong>graph learning</strong> (GNNs & Transformers), and  -->
          <!-- Now I care the most about  -->
          <!-- the following learning scenarios: -->
          <ul>
            <li><strong style="color:#94070a">Self-supervised Learning (SSL).</strong> SSL is the driving engine for pretraining foundation models. I am mostly interested in <strong>theoretical understanding and principled design of SSL</strong> (including generative and non-generative models), aimed at uncovering their inherent mechanisms and enhancing model interpretability.
               <!-- work and how to design better alternatives in principle. -->
               <!-- <strong>Self-Supervised Learning (SSL)</strong> that allows learning richful features from massive unlabeled data, laying the foundation for large-scale foundation models.</li> -->
              <!-- I am interested in understanding the mechanisms of neural networks with symmetry, a  -->
              <!-- I am interested in understanding the mechanisms of these  -->
              <!-- I am interesting in understanding and utilizing and divising equivariant NNs, with symmetry priors, such as, <strong>Graph Neural Networks</strong> and <strong>Transformers</strong>.</li> -->
            <li><strong style="color:#94070a">Safe Alignment.</strong> 
              <!-- Powerful LLMs need to be aligned to human purposes with guardrails to avoid being abused. I explore when existing alignment measures will fail (e.g., by adversarial attacks or jailbreaks), and how to systematically develop <strong>robust foundation models</strong> against adversarial and real-world distribution shifts.  -->
              Powerful LLMs need to be aligned to human purposes with guardrails to avoid being abused. I explore how to systematically develop <strong>robust foundation models</strong> against real-world distribution shifts. 
              </li>
            <li><strong style="color:#94070a">Neural Representations.</strong> I am interested in understanding the inherent representations and feature dynamics of general-purpose neural network backbones, in particular, <strong>Transformers</strong> and <strong>Graph Neural Networks</strong>.  </li>
               <!-- different data modalities (e.g., images, texts, graphs). </li> -->
          </ul>
          <!-- <strong>Note.</strong> I am open to collaboration! In the past, I have experiences mentoring talented graduate and undergraduate students --- many of my ideas emerged from these discussions.  -->
          Feel free to shoot me an email if you are interested in working with me.
         </p>

        </div>
        <div class="post-content">
          <!-- <strong> -->
          <p>
            <strong>Contact:</strong> yifei_w at mit.edu / 
            <a href="https://scholar.google.com/citations?hl=en&user=-CLy6YsAAAAJ"><span>Google Scholar</span></a> /
             <!-- <a href="assets/cv_yifei_wang_oct23.pdf"><span>CV (outdated)</span></a> / -->
             <a href="https://github.com/yifeiwang77">Github</i></span></a> / 
             <a href="https://x.com/yifeiwang77">X (Twitter)</a>
           </p>
          <!-- </strong> -->
        </div>
    
      </article>
    </div>

   <div class="wrapper">
    <article class="post"> 
        <header class="post-header">
          <h1 class="post-title">News </h1>
        </header>
        <article class="post">

          <ul>
            <li>2024.09. üéâ 6 papers were accepted to NeurIPS 2024, covering LLM self-correction, In-context SSL, Equivariant SSL, Canonization-based Equivariance, Transformer Dynamics, and Invariant Learning on Graph. </li>
            <li>2024.09. I gave a talk at NYU Tandon on <i>Building Safe Foundation Models from Principled Understanding</i>.</li>
            <li>2024.08. I will be co-organizing <a href="https://projects.csail.mit.edu/ml-tea/">ML Tea seminar</a> at MIT CSAIL this fall. <a href="https://mailman.mit.edu/mailman/listinfo/mitml">Join us</a> on the Mondays at 32-G882! </li>
            <li>2024.08. I gave a talk at Princeton University on <i>Reimagining Self-Supervised Learning with Context</i>.</li>
            <li>2024.08. I wll be serving as an Area Chair for ICLR 2025.</li>
            <li>2024.07. Our paper on <a href="https://arxiv.org/pdf/2405.18634">theoretical understanding of LLM self-correction</a> received the <strong>Spotlight Award</strong> (awarded to top 3 papers) at <a href="https://iclworkshop.github.io">ICML 2024 1st ICL Workshop</a>.</li>
            <li>2024.07. I will be co-organizing the <a href="https://redteaming-gen-ai.github.io/">NeurIPS 2024 workshop on Red Teaming GenAI</a>. </li>
            <li>2024.07. 3 papers were accepted by ICML 2024.</li> 
            <li>2024.06. I gave a talk on <i>Non-negative Contrastive Learning</i> at <a href="https://cohere.com/events/c4ai-Yifei-Wang">Cohere AI</a>. 
            <li>2024.05. I visited TU Munich and gave a talk on <i>Self-supervised Learning of Identifiable Features</i>.</li>
            <li>2024.05. I served as a Session Chair at ICLR 2024.</li>
            <li>2024.04. I gave a talk at MIT LIDS Tea  on <i>Non-negative Contrastive Learning</i> (<a href="assets/slides/NCL_LIDS_tea.pdf">slides</a>).</li>
            <!-- <li>2024.04. Anthropic proposed <a ref="https://www.anthropic.com/research/many-shot-jailbreaking">many-shot jailbreaking</a>, showing that extending our <a ref="https://arxiv.org/pdf/2310.06387.pdf">In-context Attack (ICA)</a> from a few shots (5) to many shots (256) can jailbreak most prominent LLMs (GPT-3.5/4, Claude2, Llama2-70B).  </li> -->
            <li>2024.02. I was honored to receive Wenjun Wu Outstanding Ph.D. Dissertation Runner-Up Award (top 14 nation-wide) from <a href="https://en.caai.cn/">CAAI</a>. Wenjun Wu invented <a href="https://en.wikipedia.org/wiki/Wu%27s_method_of_characteristic_set">Wu's method</a> for automatic theorem proving and pioneered AI research in China. Thanks everyone! </li>
            <li>2024.01. 3 papers were accepted to ICLR 2024.</li>
            <li>2023.12. I have joined MIT CSAIL as a postdoc.</li>
            <li>2023.09. I will be serving as an Area Chair for ICLR 2024.</li>
    </div>
    
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
          <h1 class="post-title" id="Publications">Papers <span style="font-size:14px;letter-spacing:0.00px;">(<strong>* marks equal contribution</strong>)</a></span> </h1>
        </header> 
      </article>
    <!-- Tab links -->
    <div class="tab">
      <!-- <button class="tablinks" disabled><i>Topics:</i></button> -->
      <!-- <button class="tablinks" onclick="openCity(event, 'All')"  id="defaultOpen">üìö All</button> -->
      <button class="tablinks" onclick="openCity(event, 'Selected')"   id="defaultOpen">üö© Recent</button>
      <button class="tablinks" onclick="openCity(event, 'All')">üìö All</button>
      <!-- <button class="tablinks" onclick="openCity(event, 'Unsupervised')">Topics:</button> -->
      <button class="tablinks" onclick="openCity(event, 'Unsupervised')">üéÇ Self-supervised Learning</button>
      <button class="tablinks" onclick="openCity(event, 'Adversarial')">üî® Safe Alignment</button>
      <button class="tablinks" onclick="openCity(event, 'Graph')">üåê Neural Representations</button>
    </div>

    <div id="Selected" class="tabcontent">
      <ul class="publications">


        <li class="article">
          <span class="title">
            A Theoretical Understanding of Self-Correction through In-context Alignment
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Yuyang Wu*, Zeming Wei, Stefanie Jegelka, Yisen Wang
          </span>
          <span class="journal-info">NeurIPS 2024<br> ICML 2024 Workshop on In-Context Learning (ICL) <a href="https://iclworkshop.github.io"><span class="oral">Spotlight Award (awarded to top 3 papers)</span></a> </span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2405.18634">PDF</a>
          </span>
          </li>


          <li class="article">
            <span class="title">
              Understanding the Role of Equivariance in Self-supervised Learning
            </span>
            <span class="authors"><strong>Yifei Wang*</strong>, Kaiwen Hu*, Sharut Gupta, Ziyu Ye, Yisen Wang, Stefanie Jegelka
            </span>
            <span class="journal-info">NeurIPS 2024</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://openreview.net/pdf?id=yLpuruMZHE">PDF</a>
            </span>
            </li>
  
          <li class="article">
            <span class="title">
              In-Context Symmetries: Self-Supervised Learning through Contextual World Models
            </span>
            <span class="authors">Sharut Gupta*, Chenyu Wang*, <strong>Yifei Wang*</strong>, Tommi Jaakkola, Stefanie Jegelka</span>
            <span class="journal-info">NeurIPS 2024</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2405.18193">PDF</a>
            </span>
            </li>          

            <li class="article">
              <span class="title">
                A Canonization Perspective on Invariant and Equivariant Learning
              </span>
              <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Derek Lim, Stefanie Jegelka, Yisen Wang
              </span>
              <span class="journal-info">NeurIPS 2024</span>
              <span class="year">2024</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2405.18378">PDF</a>
              </span>
              </li>  
  
              <li class="article">
                <span class="title">
                  On the Role of Attention Masks and LayerNorm in Transformers
                </span>
                <span class="authors">Xinyi Wu, Amir Ajorlou, <strong>Yifei Wang</strong>, Stefanie Jegelka, Ali Jadbabaie
                </span>
                <span class="journal-info">NeurIPS 2024</span>
                <span class="year">2024</span>
                <span class="links">
                  <a href="https://arxiv.org/pdf/2405.18781">PDF</a>
                </span>
                </li>  

                <li class="article">
                  <span class="title">
                    Dissecting the Failure of Invariant Learning on Graphs
                  </span>
                  <span class="authors">Qixun Wang, <strong>Yifei Wang</strong>, Yisen Wang, Xianghua Ying
                  </span>
                  <span class="journal-info">NeurIPS 2024</span>
                  <span class="year">2024</span>
                  <span class="links">
                    <a href="">PDF</a>
                  </span>
                  </li>                
            <!-- <li class="article">
              <span class="title">
                Rethinking Invariance in In-context Learning
              </span>
              <span class="authors">Lizhe Fang*, <strong>Yifei Wang*</strong>, Khashayar Gatmiry, Lei Fang, Yisen Wang
              </span>
              <span class="journal-info">ICML 2024 Workshop on Theoretical Foundations of Foundation Models (TF2M)</span>
              <span class="year">2024</span>
              <span class="links">
                <a href="https://openreview.net/pdf?id=xSDqxxILWg">PDF</a>
              </span>
              </li> -->

              <!-- <li class="article">
                <span class="title">
                  Non-negative Contrastive Learning
                </span>
                <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yaoyu Guo, Yisen Wang</span>
                <span class="journal-info">ICLR 2024</span>
                <span class="year">2024</span>
                <span class="links">
                  <a href="https://arxiv.org/pdf/2403.12459">PDF</a> | 
                  <a href="https://github.com/PKU-ML/non_neg">Code</a> |
                  <a href="assets/slides/NCL_LIDS_tea.pdf">Slides</a>
                </span>
                </li>    

          <li class="article">
            <span class="title">
              A Message Passing Perspective on Learning Dynamics of Contrastive Learning
            </span>
            <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, <u>Tianqi Du</u>, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
            <span class="journal-info">ICLR 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">PDF</a> |
              <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning">Code</a> |
              <a href="assets/slides/ICLR23_Message_Passing.pdf">Slides</a> |
              <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg">Blog</a>
            </span>
        </li>
        <li class="article">
          <span class="title">
            Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism
          </span>
          <span class="authors"><u>Zhijian Zhuo*</u>, <strong>Yifei Wang*</strong>, Jinwen Ma, Yisen Wang </span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">PDF</a> | 
            <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism">Code</a>
          </span>
      </li>
  
        <li class="article">
          <span class="title">
          How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
          </span>
          <span class="oral">SPOTLIGHT</span>              
          <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
          <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
            <a href="https://github.com/zhangq327/U-MAE">Code</a> |
            <a href="assets/slides/NeurIPS2022_mae.pdf">Slides</a>
          </span>
      </li>        
        <li class="article">
          <span class="title">
          Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">ICLR 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
            <a href="https://github.com/zhangq327/ARC">Code</a> |
            <a href="assets/slides/ICLR2022_overlap.pdf">Slides</a>
          </span>
        </li> 
-->

    </div>

    <div id="All" class="tabcontent">
      <ul class="publications">


        <li class="article">
          <span class="title">
            A Theoretical Understanding of Self-Correction through In-context Alignment
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Yuyang Wu*, Zeming Wei, Stefanie Jegelka, Yisen Wang
          </span>
          <span class="journal-info">NeurIPS 2024<br> ICML 2024 Workshop on In-Context Learning (ICL) <a href="https://iclworkshop.github.io"><span class="oral">Spotlight Award (awarded to top 3 papers)</span></a> </span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2405.18634">PDF</a>
          </span>
          </li>


          <li class="article">
            <span class="title">
              Understanding the Role of Equivariance in Self-supervised Learning
            </span>
            <span class="authors"><strong>Yifei Wang*</strong>, Kaiwen Hu*, Sharut Gupta, Ziyu Ye, Yisen Wang, Stefanie Jegelka
            </span>
            <span class="journal-info">NeurIPS 2024</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://openreview.net/pdf?id=yLpuruMZHE">PDF</a>
            </span>
            </li>
  
          <li class="article">
            <span class="title">
              In-Context Symmetries: Self-Supervised Learning through Contextual World Models
            </span>
            <span class="authors">Sharut Gupta*, Chenyu Wang*, <strong>Yifei Wang*</strong>, Tommi Jaakkola, Stefanie Jegelka</span>
            <span class="journal-info">NeurIPS 2024</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2405.18193">PDF</a>
            </span>
            </li>          

        <li class="article">
          <span class="title">
            Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining
          </span>
          <span class="authors">Qi Zhang, Tianqi Du, Haotian Huang, <strong>Yifei Wang</strong>, Yisen Wang</span>
          <span class="journal-info">ICML 2024</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=2rPoTgEmjV">PDF</a> |
            <a href="https://github.com/PKU-ML/LookAheadLookAround">Code</a>
          </span>
          </li>
  

        <li class="article">
          <span class="title">
            OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift
          </span>
          <span class="authors">Lin Li, <strong>Yifei Wang</strong>, Chawin Sitawarin, Michael W. Spratling</span>
          <span class="journal-info">ICML 2024</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.12793">PDF</a> |
            <a href="https://github.com/OODRobustBench/OODRobustBench">Code</a> |
            <a href="https://oodrobustbench.github.io/"><strong>Leaderboard</strong></a>
          </span>
          </li>
    

        <li class="article">
          <span class="title">
            On the Duality Between Sharpness-Aware Minimization and Adversarial Training
          </span>
          <span class="authors">Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, <strong>Yifei Wang</strong>, Zeming Wei</span>
          <span class="journal-info">ICML 2024</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/abs/2402.15152">PDF</a> | 
            <a here="https://github.com/weizeming/SAM_AT">Code</a>
          </span>
          </li>
  

        <li class="article">
          <span class="title">
            Non-negative Contrastive Learning
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yaoyu Guo, Yisen Wang</span>
          <span class="journal-info">ICLR 2024</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2403.12459">PDF</a> | 
            <a href="https://github.com/PKU-ML/non_neg">Code</a> |
            <a href="assets/slides/NCL_LIDS_tea.pdf">Slides</a>
          </span>
          </li>    

      <li class="article">
        <span class="title">
          Do Generated Data Always Help Contrastive Learning?
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, Jizhe Zhang*, Yisen Wang</span>
        <span class="journal-info">ICLR 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2403.12448.pdf">PDF</a> | 
          <a href="https://github.com/PKU-ML/adainf">Code</a> |
          <a href="https://mp.weixin.qq.com/s/MSSzIl3KnvRzgWVN0ZyW6A">Featured on Sync (CN)</a>

        <li class="article">
          <span class="title">
            On the Role of Discrete Tokenization in Visual Representation Learning
          </span>
          <span class="authors">Tianqi Du*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">ICLR 2024 <strong style="color:#94070a">(Spotlight)</strong></span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=WNLAkjUm19">PDF</a> |
            <a href="https://github.com/PKU-ML/ClusterMIM">Code</a>
          </span>
          </li>

          <li class="article">
            <span class="title">
              How to Craft Backdoors with Unlabeled Data Alone?
            </span>
            <span class="authors"><strong>Yifei Wang*</strong>, Wenhan Ma*, Stefanie Jegelka, Yisen Wang</span>
            <span class="journal-info">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM)</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2404.06694.pdf">PDF</a>
            </span>
            </li>
  
            <li class="article">
              <span class="title">
                Virtual Classifier: A Reversed Approach for Robust Image Evaluation
              </span>
              <span class="authors">Jizhe Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
              <span class="journal-info">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM)</span>
              <span class="year">2024</span>
              <span class="links">
                <a href="https://openreview.net/pdf?id=IE6FbueT47">PDF</a>
              </span>
              </li>
  
    

          <li class="article">
            <span class="title">
              Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective
            </span>
            <span class="authors"><strong>Yifei Wang*</strong>, Liangchen Li*, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2310.19360.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ReBAT">Code</a>          
            </span>
            </li>
    
          <li class="article">
          <span class="title">
            Adversarial Examples Are Not Real Features 
          </span>
          <span class="authors">Ang Li*, <strong>Yifei Wang*</strong>, Yiwen Guo, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18936.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/AdvNotRealFeatures">Code</a>          
          </span>
          </li>

        <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>

        <li class="article">
          <span class="title">
            Identifiable Contrastive Learning with Automatic Feature Importance Discovery
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18904.pdf">PDF</a> | 
            <a href="https://github.com/PKU-ML/Tri-factor-Contrastive-Learning">Code</a>            
          </span>
          </li>
  

        <li class="article">
          <span class="title">
            Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding 
          </span>
          <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18716.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/LaplacianCanonization">Code</a>          
          </span>
          </li>          
        
        <li class="article">
          <span class="title">
            On the Generalization of Multi-modal Contrastive Learning 
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">ICML 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2306.04272">PDF</a> | 
            <a href="https://github.com/PKU-ML/CLIP-Help-SimCLR">Code</a>
          </span>
          </li>


          <li class="article">
            <span class="title">
              Rethinking Weak Supervision in Helping Contrastive Representation Learning
            </span>
            <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
            <span class="journal-info">ICML 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2306.04160">PDF</a>
            </span>
          </li>

            <li class="article">
              <span class="title">
                CFA: Class-wise Calibrated Fair Adversarial Training
              </span>
              <span class="authors">Zeming Wei, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
              <span class="journal-info">CVPR 2023</span>
              <span class="year">2023</span>
              <span class="links">
                <a href="https://arxiv.org/pdf/2303.14460.pdf">PDF</a> |
                <a href="https://github.com/PKU-ML/CFA">Code</a>
              </span>
        
        <li class="article">
          <span class="title">
            Equilibrium Image Denoising with Implicit Differentiation
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Zhengyang Geng, Yisen Wang, Jiansheng Yang, and Zhouchen Lin</span>
          <span class="journal-info">IEEE Transactions on Image Processing (TIP)</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://zhouchenlin.github.io/Publications/2023-TIP-Denoising.pdf">PDF</a>
          </span>
      </li>

        <li class="article">
          <span class="title">
            A Message Passing Perspective on Learning Dynamics of Contrastive Learning
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, <u>Tianqi Du</u>, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">PDF</a> |
            <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning">Code</a> |
            <a href="assets/slides/ICLR23_Message_Passing.pdf">Slides</a> |
            <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg">Blog</a>
          </span>
      </li>

      <li class="article">
        <span class="title">
          Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism
        </span>
        <span class="authors"><u>Zhijian Zhuo*</u>, <strong>Yifei Wang*</strong>, Jinwen Ma, Yisen Wang </span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">PDF</a> | 
          <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism">Code</a>
        </span>
    </li>

      <li class="article">
        <span class="title">
          Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
        </span>
        <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a> |
          <a href="https://github.com/PKU-ML/DynACL">Code</a>
        </span>
    </li>

    <li class="article">
      <span class="title">
        ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
      </span>
      <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a> | 
        <a href="https://github.com/PKU-ML/ContraNorm">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States
      </span>
      <span class="authors">Mingjie Li, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=j3cUWIMsFBN">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        What Contrastive Learning Learns Beyond Class-wise Features?
      </span>
      <span class="authors">Xingyuming Liu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=T-NiH_wB1O">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Rethinking the Necessity of Labels in Backdoor Defense 
      </span>
      <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
      </span>
    </li>


      <li class="article">
        <span class="title">
          On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
        </span>
        <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
        <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
        </span>
    </li>
     <li class="article">
        <span class="title">
        How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
        </span>
        <!-- <span class="oral">SPOTLIGHT</span>               -->
        <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
        <span class="year">2022</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
          <a href="https://github.com/zhangq327/U-MAE">Code</a> |
          <a href="assets/slides/NeurIPS2022_mae.pdf">Slides</a>
        </span>
    </li>
    <li class="article">
      <span class="title">
      Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
        <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
        <a href="assets/slides/NeurIPS2022_OOD.pdf">Slides</a>
      </span>
    </li>

    <li class="article">
    <span class="title">
      When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
      </span>
      <!-- <span class="oral">SPOTLIGHT</span>               -->
      <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
        <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop
      </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
      AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
      </span>
      <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jianlong Chang, Qi Tian, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">IEEE BigData 2022 <span class="oral">(Long Talk)</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="assets/papers/IEEE-BigData22-VEQ.pdf">PDF</a>
      </span>
    </li>
    
    <li class="article">
      <span class="title">
        Optimization-Induced Graph Implicit Nonlinear Diffusion
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
        <a href="https://github.com/7qchen/GIND">Code</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
        <i class='no-italics' ></i>
      </span>
      <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
      <span class="journal-info">ICML 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
    </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="assets/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
      </span>         
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
        <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
      </span>
    </li>
    <!-- <li class="article">
      <span class="title">
        Fooling Adversarial Training with Inducing Noise
        <i class='no-italics'></i>
      </span>
      <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">Tech report, Nov. 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
      </span>
    </li> -->
      <li class="article">
      <span class="title">
      Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
        <a href="assets/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
      </span>
    </li>
    <li class="article">
      <span class="title">
        Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">NeurIPS 2021</span>
      <span class="year">2021</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
        <a href="https://github.com/yifeiwang77/DGC">Code</a> |
        <a href="assets/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
      </span>
      <!-- <span class="intro">A properly designed linear GCN (from a <strong>continuous</strong> perspective) is on par with SOTA nonlinear GCNs while being 100x faster => <strong>Unsupervised linear features</strong> can be astonishingly effective.</span> -->
    </li>
    <li class="article">
      <span class="title">
      Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
      </span>
                  
      <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info"> ECML-PKDD 2021 
        </span>
      <span class="year">2021</span>
      <span class="oral">(üèÜ Best ML Paper Award (1/685) & Invited to <strong>Machine Learning</strong> Journal)</span>              
      <span class="links">
        <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
        <a href="https://github.com/yifeiwang77/repgan">Code</a> |
        <a href="assets/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
        <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
        <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
        <a href="https://ecmlpkdd.org/2021/">Award</a>
      </span>
      <!-- <span class="intro">Efficient high-dimensional MCMC by reparameterizing Markov transitions into the <strong>low-dimensional latent space </strong> through implicit models (GANs). </span> -->
    </li>
    <li class="article">
      <span class="title">
      Train Once, and Decode as You Like <i class='no-italics'></i>
      </span>
      <span class="authors">Chao Tian, <strong>Yifei Wang</strong>, Hao Cheng, Yijiang Lian, Zhihua Zhang</span>
      <span class="journal-info">COLING 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://www.aclweb.org/anthology/2020.coling-main.25.pdf">PDF</a> 
      </span>
      <!-- <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> -->
    </li>
    <!-- <li class="article">
      <span class="title">
        Decoder-free Robustness Disentanglement without (Additional) Supervision
        <i class='no-italics'></i>
      </span>
      <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
      <span class="journal-info">Tech report, July 2020</span>
      <span class="year">2020</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
      </span>
       <span class="intro">Train an autoregressive language model with random ordering masks, and you can decode with whatever order (forward or backward) & whatever decoding steps <strong> (full-, semi-, or non- autoregressive). </span> 
    </li> -->
  </ul>
</div>

<div id="Unsupervised" class="tabcontent">
  <ul class="publications">


    <li class="article">
      <span class="title">
        A Theoretical Understanding of Self-Correction through In-context Alignment
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Yuyang Wu*, Zeming Wei, Stefanie Jegelka, Yisen Wang
      </span>
      <span class="journal-info">NeurIPS 2024<br> ICML 2024 Workshop on In-Context Learning (ICL) <a href="https://iclworkshop.github.io"><span class="oral">Spotlight Award (awarded to top 3 papers)</span></a> </span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2405.18634">PDF</a>
      </span>
      </li>


      <li class="article">
        <span class="title">
          Understanding the Role of Equivariance in Self-supervised Learning
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, Kaiwen Hu*, Sharut Gupta, Ziyu Ye, Yisen Wang, Stefanie Jegelka
        </span>
        <span class="journal-info">NeurIPS 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=yLpuruMZHE">PDF</a>
        </span>
        </li>

      <li class="article">
        <span class="title">
          In-Context Symmetries: Self-Supervised Learning through Contextual World Models
        </span>
        <span class="authors">Sharut Gupta*, Chenyu Wang*, <strong>Yifei Wang*</strong>, Tommi Jaakkola, Stefanie Jegelka</span>
        <span class="journal-info">NeurIPS 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2405.18193">PDF</a>
        </span>
        </li>          


    <li class="article">
      <span class="title">
        Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining
      </span>
      <span class="authors">Qi Zhang, Tianqi Du, Haotian Huang, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICML 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=2rPoTgEmjV">PDF</a> |
        <a href="https://github.com/PKU-ML/LookAheadLookAround">Code</a>
      </span>
      </li>
      <li class="article">
        <span class="title">
          Rethinking Invariance in In-context Learning
        </span>
        <span class="authors">Lizhe Fang*, <strong>Yifei Wang*</strong>, Khashayar Gatmiry, Lei Fang, Yisen Wang
        </span>
        <span class="journal-info">ICML 2024 Workshop on Theoretical Foundations of Foundation Models (TF2M)</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=xSDqxxILWg">PDF</a>
        </span>
        </li>
    
    <li class="article">
      <span class="title">
        Non-negative Contrastive Learning
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Qi Zhang*, Yaoyu Guo, Yisen Wang</span>
      <span class="journal-info">ICLR 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2403.12459">PDF</a> | 
        <a href="https://github.com/PKU-ML/non_neg">Code</a> |
        <a href="assets/slides/NCL_LIDS_tea.pdf">Slides</a>
      </span>
      </li>

    <li class="article">
      <span class="title">
        Do Generated Data Always Help Contrastive Learning?
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Jizhe Zhang*, Yisen Wang</span>
      <span class="journal-info">ICLR 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2403.12448.pdf">PDF</a> | 
        <a href="https://github.com/PKU-ML/adainf">Code</a> |
        <a href="https://mp.weixin.qq.com/s/MSSzIl3KnvRzgWVN0ZyW6A">Featured on Sync (CN)</a>
      </span>
      </li>

      <li class="article">
        <span class="title">
          On the Role of Discrete Tokenization in Visual Representation Learning
        </span>
        <span class="authors">Tianqi Du*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
        <span class="journal-info">ICLR 2024 <strong style="color:#94070a">(Spotlight)</strong></span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=WNLAkjUm19">PDF</a> |
          <a href="https://github.com/PKU-ML/ClusterMIM">Code</a>
        </span>
        </li>

        <li class="article">
          <span class="title">
            How to Craft Backdoors with Unlabeled Data Alone?
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Wenhan Ma*, Stefanie Jegelka, Yisen Wang</span>
          <span class="journal-info">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM)</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2404.06694.pdf">PDF</a>
          </span>
          </li>

          <li class="article">
            <span class="title">
              Virtual Classifier: A Reversed Approach for Robust Image Evaluation
            </span>
            <span class="authors">Jizhe Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
            <span class="journal-info">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM)</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://openreview.net/pdf?id=IE6FbueT47">PDF</a>
            </span>
            </li>


          <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>
  

        <li class="article">
          <span class="title">
            Identifiable Contrastive Learning with Automatic Feature Importance Discovery
          </span>
          <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18904.pdf">PDF</a> | 
            <a href="https://github.com/PKU-ML/Tri-factor-Contrastive-Learning">Code</a>            
          </span>
          </li>

    <li class="article">
      <span class="title">
        On the Generalization of Multi-modal Contrastive Learning 
      </span>
      <span class="authors">Qi Zhang*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
      <span class="journal-info">ICML 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2306.04272">PDF</a> | 
        <a href="https://github.com/PKU-ML/CLIP-Help-SimCLR">Code</a>
      </span>
      </li>


      <li class="article">
        <span class="title">
          Rethinking Weak Supervision in Helping Contrastive Representation Learning
        </span>
        <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
        <span class="journal-info">ICML 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2306.04160">PDF</a>
        </span>
      </li>
    
      <li class="article">
        <span class="title">
          A Message Passing Perspective on Learning Dynamics of Contrastive Learning
        </span>
        <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, <u>Tianqi Du</u>, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
        <span class="journal-info">ICLR 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://openreview.net/pdf?id=VBTJqqWjxMv">PDF</a> |
          <a href="https://github.com/PKU-ML/Message-Passing-Contrastive-Learning">Code</a> |
          <a href="assets/slides/ICLR23_Message_Passing.pdf">Slides</a> |
          <a href="https://mp.weixin.qq.com/s/e18pZfee7ffwAHMUBZ_OEg">Blog</a>          
        </span>
    </li>

    <li class="article">
      <span class="title">
        Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism
      </span>
      <span class="authors"><u>Zhijian Zhuo*</u>, <strong>Yifei Wang*</strong>, Jinwen Ma, Yisen Wang </span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=cIbjyd2Vcy">PDF</a> | 
        <a href="https://github.com/PKU-ML/Rank-Differential-Mechanism">Code</a>
      </span>
  </li>

    <li class="article">
      <span class="title">
        Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
      </span>
      <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a> |
        <a href="https://github.com/PKU-ML/DynACL">Code</a>
      </span>
  </li>

  <li class="article">
    <span class="title">
      ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
    </span>
    <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
    <span class="journal-info">ICLR 2023</span>
    <span class="year">2023</span>
    <span class="links">
      <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a> | 
      <a href="https://github.com/PKU-ML/ContraNorm">Code</a>
    </span>
  </li>

      <li class="article">
      <span class="title">
        What Contrastive Learning Learns Beyond Class-wise Features?
      </span>
      <span class="authors">Xingyuming Liu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=T-NiH_wB1O">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
        Rethinking the Necessity of Labels in Backdoor Defense 
      </span>
      <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
      </span>
    </li>

    <li class="article">
      <span class="title">
      How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders
      </span>
      <span class="authors"><u>Qi Zhang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span>   </span>
      <span class="year">2022</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2210.08344">PDF</a> |
        <a href="https://github.com/zhangq327/U-MAE">Code</a> |
        <a href="assets/slides/NeurIPS2022_mae.pdf">Slides</a>
      </span>
  </li>

  <li class="article">
    <span class="title">
      Variational Energy-Based Models: A Probabilistic Framework for Contrastive Self-Supervised Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Tianqi Du</u>*, <strong>Yifei Wang*</strong>, Weiran Huang, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop
    </span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_64.pdf">PDF</a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    AggNCE: Asymptotically Identifiable Contrastive Learning <i class='no-italics'>  </i>
    </span>
    <span class="authors">Jingyi Cui*, Weiran Huang*, <strong>Yifei Wang</strong>, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 SSL Workshop <span class="oral">(Oral)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://sslneurips22.github.io/paper_pdfs/paper_68.pdf">PDF</a>
    </span>
  </li>

    <li class="article">
      <span class="title">
      Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap <i class='no-italics' ></i>
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, <u>Qi Zhang</u>*, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
      <span class="journal-info">ICLR 2022</span>
      <span class="year">2022</span>
      <span class="links">
        <a href="http://arxiv.org/pdf/2203.13457">PDF</a> |
        <a href="https://github.com/zhangq327/ARC">Code</a> |
        <a href="assets/slides/ICLR2022_overlap.pdf">Slides</a>
      </span>
    </li>

  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>

  <li class="article">
    <span class="title">
    Residual Relaxation for Multi-view Representation Learning <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">NeurIPS 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2110.15348">PDF</a> |
      <a href="assets/slides/NeurIPS2021_Prelax_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/AT9kWTNiImwS-Ns-zM5pCw">Blog </a>
    </span>
  </li>

  <li class="article">
    <span class="title">
    Reparameterized Sampling for Generative Adversarial Networks <i class='no-italics'> </i>
    </span>
                
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info"> ECML-PKDD 2021 
      </span>
    <span class="year">2021</span>
    <span class="oral">(üèÜ Best ML Paper Award (1/685). Invited to Machine Learning Journal)</span>              
    <span class="links">
      <a href="https://arxiv.org/pdf/2107.00352">PDF</a> |
      <a href="https://github.com/yifeiwang77/repgan">Code</a> |
      <a href="assets/slides/ECML2021_REPGAN_slides.pdf">Slides</a> |
      <a href="https://mp.weixin.qq.com/s/Ah43Tqhn0CL2kLxhI_nieQ">Media </a> |
      <a href="https://www.bilibili.com/video/BV1sL4y167gj">Talk </a> |
      <a href="https://ecmlpkdd.org/2021/">Award</a>
    </span>
  </li>
</ul>
</div>


<div id="Adversarial" class="tabcontent">  
  <ul class="publications">

    <li class="article">
      <span class="title">
        A Theoretical Understanding of Self-Correction through In-context Alignment
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Yuyang Wu*, Zeming Wei, Stefanie Jegelka, Yisen Wang
      </span>
      <span class="journal-info">NeurIPS 2024<br> ICML 2024 Workshop on In-Context Learning (ICL) <a href="https://iclworkshop.github.io"><span class="oral">Spotlight Award (top 3 papers)</span></a> </span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2405.18634">PDF</a>
      </span>
      </li>


    <li class="article">
      <span class="title">
        OODRobustBench: a benchmark and large-scale analysis of adversarial robustness under distribution shift
      </span>
      <span class="authors">Lin Li, <strong>Yifei Wang</strong>, Chawin Sitawarin, Michael W. Spratling</span>
      <span class="journal-info">ICML 2024</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2310.12793">PDF</a> |
        <a href="https://github.com/OODRobustBench/OODRobustBench">Code</a> |
        <a href="https://oodrobustbench.github.io/"><strong>Leaderboard</strong></a>
      </span>
      </li>

      <li class="article">
        <span class="title">
          On the Duality Between Sharpness-Aware Minimization and Adversarial Training
        </span>
        <span class="authors">Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, <strong>Yifei Wang</strong>, Zeming Wei</span>
        <span class="journal-info">ICML 2024</span>
        <span class="year">2024</span>
        <span class="links">
          <a href="https://arxiv.org/abs/2402.15152">PDF</a> | 
          <a here="https://github.com/weizeming/SAM_AT">Code</a>
        </span>
        </li>

    <li class="article">
      <span class="title">
        How to Craft Backdoors with Unlabeled Data Alone?
      </span>
      <span class="authors"><strong>Yifei Wang*</strong>, Wenhan Ma*, Stefanie Jegelka, Yisen Wang</span>
      <span class="journal-info">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM)</span>
      <span class="year">2024</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2404.06694.pdf">PDF</a>
      </span>
      </li>


      <li class="article">
        <span class="title">
          Adversarial Examples Are Not Real Features 
        </span>
        <span class="authors">Ang Li*, <strong>Yifei Wang*</strong>, Yiwen Guo, Yisen Wang </span>
        <span class="journal-info">NeurIPS 2023</span>
        <span class="year">2023</span>
        <span class="links">
          <a href="https://arxiv.org/pdf/2310.18936.pdf">PDF</a> |
          <a href="https://github.com/PKU-ML/AdvNotRealFeatures">Code</a>          
        </span>
        </li>

        <li class="article">
          <span class="title">
            Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective
          </span>
          <span class="authors"><strong>Yifei Wang*</strong>, Liangchen Li*, Jiansheng Yang, Zhouchen Lin, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.19360.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/ReBAT">Code</a>          
          </span>
          </li>


    <li class="article">
      <span class="title">
        CFA: Class-wise Calibrated Fair Adversarial Training
      </span>
      <span class="authors">Zeming Wei, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
      <span class="journal-info">CVPR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2303.14460.pdf">PDF</a> |
                <a href="https://github.com/PKU-ML/CFA">Code</a>
      </span>
  
    <li class="article">
      <span class="title">
        Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning
      </span>
      <span class="authors"><u>Rundong Luo</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
      <span class="journal-info">ICLR 2023</span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://openreview.net/pdf?id=0qmwFNJyxCL">PDF</a>
      </span>
  </li>        

  <li class="article">
    <span class="title">
      Rethinking the Necessity of Labels in Backdoor Defense 
    </span>
    <span class="authors">Zidi Xiong, Dongxian Wu, <strong>Yifei Wang</strong>, Yisen Wang</span>
    <span class="journal-info">ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning (BANDS)</span>
    <span class="year">2023</span>
    <span class="links">
      <a href="https://openreview.net/pdf?id=Noj1Fydegod">PDF</a>
    </span>
  </li>

    <li class="article">
      <span class="title">
        On the Connection between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization
      </span>
      <span class="authors"><u>Shiji Xin</u>, <strong>Yifei Wang</strong>, Jingtong Su, Yisen Wang</span>
      <span class="journal-info">AAAI 2023 <span class="oral">(Oral)</span></span>
      <span class="year">2023</span>
      <span class="links">
        <a href="https://arxiv.org/pdf/2212.09082.pdf">PDF</a>
      </span>
  </li>
  <li class="article">
    <span class="title">
    Improving Out-of-distribution Robustness by Adversarial Training with Structured Priors <i class='no-italics'>  </i>
    </span>
    <span class="authors"><u>Qixun Wang</u>*, <strong>Yifei Wang*</strong>, Hong Zhu, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.06807">PDF</a> |
      <a href="https://github.com/NOVAglow646/NIPS22-MAT-and-LDAT-for-OOD">Code</a> |
      <a href="assets/slides/NeurIPS2022_OOD.pdf">Slides</a>
    </span>
  </li>

  <li class="article">
  <span class="title">
    When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture  <i class='no-italics'>  </i>
    </span>
    <!-- <span class="oral">SPOTLIGHT</span>               -->
    <span class="authors"><u>Yichuan Mo</u>, Dongxian Wu, <strong>Yifei Wang</strong>, Yiwen Guo, Yisen Wang</span>
    <span class="journal-info">NeurIPS 2022 <span class="oral">(Spotlight, Top 5%)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2210.07540.pdf">PDF</a> |
      <a href="https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers">Code</a>
    </span>
  </li>
  <li class="article">
    <span class="title">
      A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training <i class='no-italics'>  </i>
    </span>         
    <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
    <span class="journal-info">ICLR 2022 <span class="oral">(üèÜ Silver Best Paper Award @ ICML 2021 AdvML workshop)</span></span>
    <span class="year">2022</span>
    <span class="links">
      <a href="http://arxiv.org/pdf/2203.13455">PDF</a> |
      <a href="assets/slides/ICLR2022_CEM.pdf">Slides</a> | <a href="https://advml-workshop.github.io/icml2021/">Award</a> 
    </span>
  </li>
  <!-- <li class="article">
    <span class="title">
      Fooling Adversarial Training with Inducing Noise
      <i class='no-italics'></i>
    </span>
    <span class="authors"> <u>Zhirui Wang</u>*, <strong>Yifei Wang*</strong>, Yisen Wang</span>
    <span class="journal-info">Tech report, Nov. 2021</span>
    <span class="year">2021</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2111.10130">PDF</a> 
    </span>
  </li> -->
  <!-- <li class="article">
    <span class="title">
      Decoder-free Robustness Disentanglement without (Additional) Supervision
      <i class='no-italics'></i>
    </span>
    <span class="authors"><strong>Yifei Wang</strong>, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, Jiansheng Yang</span>
    <span class="journal-info">Tech report, July 2020</span>
    <span class="year">2020</span>
    <span class="links">
      <a href="https://arxiv.org/pdf/2007.01356">PDF</a> 
    </span>
  </li> -->
  </ul>
    </div>
    <div id="Graph" class="tabcontent">      
      <ul class="publications">

        <li class="article">
          <span class="title">
            A Canonization Perspective on Invariant and Equivariant Learning
          </span>
          <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Derek Lim, Stefanie Jegelka, Yisen Wang
          </span>
          <span class="journal-info">NeurIPS 2024</span>
          <span class="year">2024</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2405.18378">PDF</a>
          </span>
          </li>  

          <li class="article">
            <span class="title">
              On the Role of Attention Masks and LayerNorm in Transformers
            </span>
            <span class="authors">Xinyi Wu, Amir Ajorlou, <strong>Yifei Wang</strong>, Stefanie Jegelka, Ali Jadbabaie
            </span>
            <span class="journal-info">NeurIPS 2024</span>
            <span class="year">2024</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2405.18781">PDF</a>
            </span>
            </li>  

            <li class="article">
              <span class="title">
                Dissecting the Failure of Invariant Learning on Graphs
              </span>
              <span class="authors">Qixun Wang, <strong>Yifei Wang</strong>, Yisen Wang, Xianghua Ying
              </span>
              <span class="journal-info">NeurIPS 2024</span>
              <span class="year">2024</span>
              <span class="links">
                <a href="">PDF</a>
              </span>
              </li>                

            <li class="article">
          <span class="title">
            Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding 
          </span>
          <span class="authors">George Ma*, <strong>Yifei Wang*</strong>, Yisen Wang </span>
          <span class="journal-info">NeurIPS 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2310.18716.pdf">PDF</a> |
            <a href="https://github.com/PKU-ML/LaplacianCanonization">Code</a>          
          </span>
          </li>

          <li class="article">
            <span class="title">
              Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
            </span>
            <span class="authors">Xiaojun Guo*, <strong>Yifei Wang*</strong>, Zeming Wei, Yisen Wang </span>
            <span class="journal-info">NeurIPS 2023</span>
            <span class="year">2023</span>
            <span class="links">
              <a href="https://arxiv.org/pdf/2311.02687.pdf">PDF</a> |
              <a href="https://github.com/PKU-ML/ArchitectureMattersGCL">Code</a>
            </span>
            </li>
  


        <li class="article">
          <span class="title">
            ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond
          </span>
          <span class="authors"><u>Xiaojun Guo</u>*, <strong>Yifei Wang*</strong>, <u>Tianqi Du*</u>, Yisen Wang</span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=SM7XkJouWHm">PDF</a>
          </span>
        </li>
    
        <li class="article">
          <span class="title">
            Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States
          </span>
          <span class="authors">Mingjie Li, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
          <span class="journal-info">ICLR 2023</span>
          <span class="year">2023</span>
          <span class="links">
            <a href="https://openreview.net/pdf?id=j3cUWIMsFBN">PDF</a>
          </span>
        </li>
    
        <li class="article">
          <span class="title">
            Efficient and Scalable Implicit Graph Neural Networks with Virtual Equilibrium
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jianlong Chang, Qi Tian, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">IEEE BigData 2022 <span class="oral">(Long Talk)</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="assets/papers/IEEE-BigData22-VEQ.pdf">PDF</a>
          </span>
        </li>

        <li class="article">
          <span class="title">
            Optimization-Induced Graph Implicit Nonlinear Diffusion
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Qi Chen, <strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/chen22z/chen22z.pdf">PDF</a> | 
            <a href="https://github.com/7qchen/GIND">Code</a>
          </span>
        </li>
      
        <li class="article">
          <span class="title">
            G<sup>2</sup>CN: Graph Gaussian Convolution Networks with Concentrated Graph Filters
            <i class='no-italics' ></i>
          </span>
          <span class="authors">Mingjie Li, <u>Xiaojun Guo</u>, <strong>Yifei Wang</strong>, Yisen Wang, Zhouchen Lin</span>
          <span class="journal-info">ICML 2022</span>
          <span class="year">2022</span>
          <span class="links">
            <a href="https://proceedings.mlr.press/v162/li22h/li22h.pdf">PDF</a>
        </li>
        <li class="article">
          <span class="title">
            Dissecting the Diffusion Process in Linear Graph Convolutional Networks <i class='no-italics'></i>
          </span>
          <span class="authors"><strong>Yifei Wang</strong>, Yisen Wang, Jiansheng Yang, Zhouchen Lin</span>
          <span class="journal-info">NeurIPS 2021</span>
          <span class="year">2021</span>
          <span class="links">
            <a href="https://arxiv.org/pdf/2102.10739">PDF</a> |
            <a href="https://github.com/yifeiwang77/DGC">Code</a> |
            <a href="assets/slides/NeurIPS2021_DGC_slides.pdf">Slides</a> |
            <a href="https://mp.weixin.qq.com/s/H5GJnsc3F-qFAUPuZBJ4gA">Blog </a>
          </span>
        </li>
      </ul>
        </div>
      </div>
    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Selected Awards</h1>
        </header>
        <div class="post-content">
          <strong><a href="hhttps://iclworkshop.github.io">Spotlight Award</a></strong>  (top 3), ICML 2024 ICL workshop, 2024.<br>
          <strong>Wenjun Wu Outstanding Ph.D. Dissertation Runner-Up Award</strong> (top 14 nation-wide), <a href="https://en.caai.cn/">CAAI</a>, 2024<br>
          <strong>Excellent Graduate</strong> (top 1 per department), Beijing, 2023<br>
          <strong>Excellent Graduate</strong>, Peking University, 2023<br>
          <strong>Baidu Scholarship Runner-Up</strong> (top 20 nation-wide), Baidu Inc, 2022<br>
          <strong>National Scholarship</strong> (top 0.1% nation-wide), China, 2021, 2022 <br>
          <strong>Principal Scholarship</strong> (top 1% university-wide), Peking University, 2022 <br>
          <strong><a href="https://ecmlpkdd.org/2021/">Best ML Paper Award</a></strong> (1/685), ECML-PKDD 2021, 2021 <br>
          <!-- <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> -->
          <strong><a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a></strong>, ICML 2021 AdvML workshop, 2021<br>
          <!-- <strong>Academic Innovation Award</strong>, Peking University, 2022. <br>   -->
          <!-- <a href="https://ecmlpkdd.org/2021/" style="color:#000000;"> -->
      </div>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Community Services</h1>
        </header>
        <div class="post-content">
    Reviewer:
      <ul>
        <li>Conferences: <strong>NeurIPS</strong> (2022, 2023, 2024), <strong>ICML</strong> (2022), <strong>AISTATS</strong> (2024, 2025), <strong>LoG</strong> (2023, 2024), <strong>ECML-PKDD</strong> (2022), AAAI (2025), <strong>CVPR</strong> (2023, 2024), <strong>ICCV</strong> (2023), <strong>ACL</strong> (2020, 2021)</li>
      <li>
        Journal: <strong>IEEE TPAMI</strong>, <strong>TMLR</strong>, <strong>TKDE</strong>
      </li>
    </ul>
    Area Chair:
    <ul>
      <li> <strong>ICLR</strong> (2024, 2025) </li>
    </ul>
    Workshop Organizer:
    <ul>
    <li><strong>NeurIPS 2024 Workshop</strong> <a href="https://redteaming-gen-ai.github.io/">Red Teaming GenAI: What Can We Learn from Adversaries</a></li>
  </ul>
    Seminar Organizer:
    <ul>
    <li><strong><a href="https://projects.csail.mit.edu/ml-tea/">ML Tea Seminar</a></strong> of MIT CSAIL (2024 Fall)</li>
  </ul>

      <!-- Reviewed for NeurIPS, ICLR, ICML, TPAMI (ML), CVPR, ICCV, ACL, EMNLP, ECML-PKDD.
      </div>
    </div>

  <!-- <div class="wrapper">
    <article class="post">
      <header class="post-header">
      <h1 class="post-title">Teaching</h1>
      </header>
      <div class="post-content">
      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>
      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>
      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>
    </div>
  </div>
</main> -->

<script>
  // Check if API exists
  if (document && document.fonts) {    
    // Do not block page loading
    setTimeout(function () {           
      document.fonts.load('16px "Mukta"').then(() => {
        // Make font using elements visible
        document.documentElement.classList.add('font-loaded') 
      })
    }, 0)
  } else {
    // Fallback if API does not exist 
    document.documentElement.classList.add('font-loaded') 
  }
</script>

<script>
  function openCity(evt, cityName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(cityName).style.display = "block";
    evt.currentTarget.className += " active";
  }
// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
  </script>

  </footer>
